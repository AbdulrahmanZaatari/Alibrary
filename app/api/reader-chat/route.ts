import { NextRequest } from 'next/server';
import { generateResponse } from '@/lib/gemini';
import { 
  getDb,
  getChatMessages,
  addChatMessage,
  updateSessionTimestamp,
  trackConversationContext,
  createSessionSummary,
  trackGlobalMemory,
  getSessionContexts
} from '@/lib/db';
import { analyzeQuery } from '@/lib/queryProcessor';
import { retrieveSmartContext } from '@/lib/smartRetrieval';
import { correctChunksBatch } from '@/lib/spellingCorrection';
import { createClient } from '@supabase/supabase-js';
import { 
  isComplexQuery, 
  performMultiHopReasoning, 
  formatMultiHopResponse 
} from '@/lib/multiHopReasoning';
import {
  analyzeConversationContext,
  generateSessionSummary as generateContextSummary,
  extractTopicsFromMessage
} from '@/lib/contextAnalyzer';

export const dynamic = 'force-dynamic';
export const runtime = 'nodejs';

const supabaseAdmin = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

/**
 * âœ… Detect document language from Supabase embeddings
 */
async function detectDocumentLanguage(documentId: string): Promise<'ar' | 'en'> {
  try {
    console.log(`ğŸ” Detecting language for document: ${documentId}`);

    const { data, error } = await supabaseAdmin
      .from('embeddings')
      .select('chunk_text')
      .eq('document_id', documentId)
      .limit(20);

    if (error) {
      console.error('âš ï¸ Error fetching embeddings:', error);
      return 'ar';
    }

    if (!data || data.length === 0) {
      console.warn('âš ï¸ No embeddings found for document, defaulting to Arabic');
      return 'ar';
    }

    const contentChunks = data.filter(row => {
      const text = row.chunk_text.toLowerCase();
      return !(
        text.includes('table of contents') ||
        text.includes('chapter') && text.length < 100 ||
        /^page \d+/i.test(text) ||
        text === 'ÙÙŠ Ø±Ø­Ø§Ø¨ Ø£Ù…Ø±ÙŠÙƒØ§'
      );
    });

    const chunksToAnalyze = contentChunks.length > 0 ? contentChunks : data;
    const combinedText = chunksToAnalyze.map(row => row.chunk_text).join(' ');
    
    const arabicChars = (combinedText.match(/[\u0600-\u06FF]/g) || []).length;
    const totalChars = combinedText.replace(/\s/g, '').length;

    const arabicRatio = arabicChars / totalChars;
    const detectedLang = arabicRatio > 0.3 ? 'ar' : 'en';

    console.log(`   âœ… Language detected: ${detectedLang} (${(arabicRatio * 100).toFixed(1)}% Arabic, analyzed ${chunksToAnalyze.length} chunks)`);

    return detectedLang;

  } catch (error) {
    console.error('âŒ Error in detectDocumentLanguage:', error);
    return 'ar';
  }
}

/**
 * âœ… Detect user's query language
 */
function detectQueryLanguage(query: string): 'ar' | 'en' {
  const arabicChars = (query.match(/[\u0600-\u06FF]/g) || []).length;
  const totalChars = query.replace(/\s/g, '').length;
  const arabicRatio = arabicChars / totalChars;
  
  return arabicRatio > 0.3 ? 'ar' : 'en';
}

/**
 * âœ… Detect languages for multiple documents
 */
async function detectMultipleDocumentLanguages(documentIds: string[]): Promise<{
  primary: 'ar' | 'en';
  languages: Map<string, 'ar' | 'en'>;
  isMultilingual: boolean;
}> {
  const languages = new Map<string, 'ar' | 'en'>();
  
  for (const docId of documentIds) {
    const lang = await detectDocumentLanguage(docId);
    languages.set(docId, lang);
  }
  
  const arabicCount = Array.from(languages.values()).filter(l => l === 'ar').length;
  const englishCount = languages.size - arabicCount;
  
  const primary = arabicCount >= englishCount ? 'ar' : 'en';
  const isMultilingual = arabicCount > 0 && englishCount > 0;
  
  console.log(`ğŸŒ Multi-document language analysis:
   - Total documents: ${documentIds.length}
   - Arabic: ${arabicCount}
   - English: ${englishCount}
   - Primary: ${primary}
   - Multilingual: ${isMultilingual}`);
  
  return { primary, languages, isMultilingual };
}

export async function POST(req: NextRequest) {
  const encoder = new TextEncoder();
  const stream = new TransformStream();
  const writer = stream.writable.getWriter();

  (async () => {
    try {
      const { 
        message, 
        query,
        sessionId, 
        bookId, 
        bookTitle, 
        bookPage,
        extractedText,
        documentIds,
        correctSpelling = false,
        aggressiveCorrection = false,
        customPrompt,
        enableMultiHop = false,
        preferredModel
      } = await req.json();

      const userMessage = message || query;

      console.log('ğŸ“š Reader Chat:', {
        sessionId,
        hasMessage: !!userMessage,
        hasCorpus: documentIds?.length > 0,
        corpusCount: documentIds?.length || 0,
        correctSpelling,
        aggressiveCorrection,
        enableMultiHop,
        preferredModel
      });

      if (!userMessage) {
        await writer.write(encoder.encode('Error: Missing message or query'));
        await writer.close();
        return;
      }

      // âœ… STEP 1: Load conversation history (if session exists)
      let history: Array<{ role: string; content: string; created_at: string }> = [];
      if (sessionId) {
        const db = getDb();
        history = db.prepare(`
          SELECT role, content, created_at
          FROM chat_messages 
          WHERE session_id = ? 
          ORDER BY created_at DESC
          LIMIT 10
        `).all(sessionId) as Array<{ role: string; content: string; created_at: string }>;
        
        history.reverse(); // Chronological order
        console.log(`ğŸ“œ Loaded ${history.length} previous messages`);
      }

      // âœ… STEP 2: Analyze conversation context (every 3 messages)
      if (sessionId && history.length > 0 && history.length % 3 === 0) {
        console.log('ğŸ§  Analyzing reader chat context...');
        
        const queryLanguage = detectQueryLanguage(userMessage);
        const conversationHistory = history.map(msg => ({
          role: msg.role,
          content: msg.content
        }));

        try {
          const context = await analyzeConversationContext(conversationHistory, queryLanguage);
          
          if (context.topics.length > 0) {
            for (const topic of context.topics.slice(0, 3)) {
              const contextId = `ctx-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
              trackConversationContext({
                id: contextId,
                sessionId,
                topic,
                keywords: context.keywords,
                entities: context.entities,
                relevanceScore: 0.8
              });
            }
          }

          if (context.mainTheme) {
            const memoryId = `mem-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
            trackGlobalMemory({
              id: memoryId,
              topic: context.mainTheme,
              context: `Book: ${bookTitle || 'Unknown'}, Page: ${bookPage || 'N/A'}, Intent: ${context.userIntent}`,
              sessionId
            });
          }

          console.log('âœ… Reader context tracked');
        } catch (error) {
          console.error('âš ï¸ Context analysis failed:', error);
        }
      }

      // âœ… STEP 3: Generate summary (every 10 messages)
      if (sessionId && history.length > 0 && history.length % 10 === 0) {
        console.log('ğŸ“ Generating reader session summary...');
        
        try {
          const queryLanguage = detectQueryLanguage(userMessage);
          const conversationHistory = history.map(msg => ({
            role: msg.role,
            content: msg.content
          }));

          const summaryResult = await generateContextSummary(conversationHistory, queryLanguage);
          
          const summaryId = `sum-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
          createSessionSummary({
            id: summaryId,
            sessionId,
            summary: summaryResult.summary,
            keyPoints: summaryResult.keyPoints,
            messageCount: history.length
          });

          console.log('âœ… Reader summary created');
        } catch (error) {
          console.error('âš ï¸ Summary generation failed:', error);
        }
      }

      // âœ… Route to appropriate handler
      if (documentIds && documentIds.length > 0) {
        console.log('ğŸ”„ Using corpus retrieval for Reader Chat');
        await handleCorpusQuery(
          writer, 
          encoder, 
          userMessage, 
          documentIds, 
          extractedText, 
          correctSpelling, 
          aggressiveCorrection,
          customPrompt,
          enableMultiHop,
          sessionId,
          history,
          bookTitle,
          bookPage,
          preferredModel
        );
      } 
      else if (sessionId) {
        console.log('ğŸ’¬ Using general chat with history for Reader Chat');
        await handleGeneralChat(writer, encoder, userMessage, sessionId, extractedText, bookPage, history);
      }
      else {
        console.log('ğŸ“ Using simple query response');
        await handleSimpleQuery(writer, encoder, userMessage, extractedText);
      }

      // âœ… STEP 4: Save user message
      if (sessionId) {
        const messageId = `msg-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
        addChatMessage({
          id: messageId,
          sessionId,
          role: 'user',
          content: userMessage,
          mode: 'reader',
          bookId,
          bookTitle,
          bookPage,
          extractedText
        });

        updateSessionTimestamp(sessionId);

        // Extract topics
        const topics = extractTopicsFromMessage(userMessage);
        if (topics.length > 0) {
          console.log('ğŸ“Œ Extracted topics:', topics);
        }
      }

      await writer.close();

    } catch (error) {
      console.error('âŒ Reader chat error:', error);
      try {
        const errorMsg = error instanceof Error ? error.message : 'Unknown error';
        await writer.write(encoder.encode(`Error: ${errorMsg}`));
        await writer.close();
      } catch {}
    }
  })();

  return new Response(stream.readable, {
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
    },
  });
}

// ==================== CORPUS QUERY HANDLER (WITH OPTIONAL MULTI-HOP) ====================
async function handleCorpusQuery(
  writer: WritableStreamDefaultWriter,
  encoder: TextEncoder,
  query: string,
  documentIds: string[],
  extractedText?: string,
  correctSpelling?: boolean,
  aggressiveCorrection?: boolean,
  customPrompt?: string,
  enableMultiHop: boolean = false,
  sessionId?: string,
  history?: Array<{ role: string; content: string }>,
  bookTitle?: string,
  bookPage?: number,
  preferredModel?: string
) {
  // âœ… Build conversation context string
  let conversationContextString = '';
  let contextualPromptAddition = '';
  
  if (sessionId && history && history.length > 0) {
    const db = getDb();
    const contexts = getSessionContexts(sessionId) as Array<{
      topic: string;
      keywords: string;
      mention_count: number;
    }>;

    if (contexts.length > 0) {
      const recentTopics = contexts
        .slice(0, 3)
        .map(c => c.topic)
        .join(', ');
      
      const queryLanguage = detectQueryLanguage(query);
      contextualPromptAddition = queryLanguage === 'ar'
        ? `\n\nğŸ“‹ **Ø§Ù„ÙˆØ¹ÙŠ Ø¨Ø§Ù„Ø³ÙŠØ§Ù‚:**\nØ§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ØªÙŠ Ù†Ø§Ù‚Ø´Ù†Ø§Ù‡Ø§ Ù…Ø¤Ø®Ø±Ø§Ù‹: ${recentTopics}\n`
        : `\n\nğŸ“‹ **Context Awareness:**\nRecent topics we've discussed: ${recentTopics}\n`;
    }

    // Build recent conversation history (last 4 messages)
    const recentHistory = history.slice(-4);
    if (recentHistory.length > 0) {
      const queryLanguage = detectQueryLanguage(query);
      conversationContextString = queryLanguage === 'ar'
        ? '\n\nğŸ“œ **Ù…Ø­Ø§Ø¯Ø«ØªÙ†Ø§ Ø§Ù„Ø£Ø®ÙŠØ±Ø©:**\n'
        : '\n\nğŸ“œ **Recent conversation:**\n';
      
      recentHistory.forEach(msg => {
        const label = msg.role === 'user' 
          ? (queryLanguage === 'ar' ? 'Ø£Ù†Øª' : 'You')
          : (queryLanguage === 'ar' ? 'Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯' : 'Assistant');
        conversationContextString += `**${label}:** ${msg.content.substring(0, 100)}${msg.content.length > 100 ? '...' : ''}\n\n`;
      });
    }
  }

  // âœ… Step 1: Detect languages for all documents
  const { primary: documentLanguage, languages: docLanguages, isMultilingual } = 
    await detectMultipleDocumentLanguages(documentIds);

  // âœ… Step 2: Detect user's query language
  const queryLanguage = detectQueryLanguage(query);
  console.log(`ğŸ—£ï¸ Query language: ${queryLanguage}`);

  // âœ… Step 3: Determine response language
  const responseLanguage = queryLanguage;
  console.log(`ğŸ’¬ Response will be in: ${responseLanguage}`);

  // âœ… Step 4: Check if query requires multi-hop reasoning (only if enabled)
  const requiresMultiHop = enableMultiHop && isComplexQuery(query);
  
  if (requiresMultiHop) {
    console.log('ğŸ§  Complex query detected - activating multi-hop reasoning');
    
    try {
      const multiHopResult = await performMultiHopReasoning(
        query,
        documentIds,
        docLanguages,
        4,
        responseLanguage,
        correctSpelling || false,
        aggressiveCorrection || false
      );
      
      // Add conversation context prefix
      let conversationPrefix = '';
      if (conversationContextString) {
        conversationPrefix = responseLanguage === 'ar'
          ? `ğŸ’­ **Ø§Ø³ØªÙƒÙ…Ø§Ù„Ø§Ù‹ Ù„Ù…Ø­Ø§Ø¯Ø«ØªÙ†Ø§:**\n\n${conversationContextString}\n\n---\n\n`
          : `ğŸ’­ **Continuing our conversation:**\n\n${conversationContextString}\n\n---\n\n`;
      }
      
      const formattedResponse = conversationPrefix + formatMultiHopResponse(multiHopResult, responseLanguage);
      await writer.write(encoder.encode(formattedResponse));
      
      console.log('âœ… Multi-hop response complete');
      return;
      
    } catch (error) {
      console.error('âŒ Multi-hop reasoning failed, falling back to standard retrieval:', error);
    }
  }

  // ==================== STANDARD RETRIEVAL (DEFAULT OR FALLBACK) ====================
  console.log(enableMultiHop ? 'ğŸ“– Using standard retrieval (fallback)' : 'ğŸ“– Using standard retrieval strategy');
  
  const contextParts: string[] = [];

  // âœ… Step 5: Analyze and translate query
  const queryAnalysis = await analyzeQuery(query, documentLanguage);
  console.log('ğŸ” Query Analysis:', {
    original: queryAnalysis.originalQuery,
    translated: queryAnalysis.translatedQuery,
    type: queryAnalysis.queryType,
    keywords: queryAnalysis.keywords,
    isMultiDoc: queryAnalysis.isMultiDocumentQuery
  });

  // âœ… Step 6: Add extracted text if provided
  if (extractedText) {
    const extractLabel = responseLanguage === 'ar' 
      ? '**ğŸ“„ Ù†Øµ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©:**'
      : '**ğŸ“„ Current Page Text:**';
    contextParts.push(`${extractLabel}\n${extractedText}`);
  }

  // âœ… Step 7: Smart corpus retrieval
  console.log('ğŸ”„ Starting smart retrieval...');
  const { chunks, strategy, confidence } = await retrieveSmartContext(queryAnalysis, documentIds);
  
  console.log(`ğŸ“Š Retrieval Results:
   - Strategy: ${strategy}
   - Chunks found: ${chunks.length}
   - Confidence: ${(confidence * 100).toFixed(1)}%`);

  // âœ… Step 8: Process chunks with optional spelling correction
  let processedChunks = chunks;
  if (correctSpelling && chunks.length > 0) {
    console.log('ğŸ”§ Applying spelling correction...');
    
    const chunksByDoc = new Map<string, any[]>();
    chunks.forEach(chunk => {
      const docId = chunk.document_id;
      if (!chunksByDoc.has(docId)) {
        chunksByDoc.set(docId, []);
      }
      chunksByDoc.get(docId)!.push(chunk);
    });

    processedChunks = [];
    for (const [docId, docChunks] of chunksByDoc.entries()) {
      const docLang = docLanguages.get(docId) || documentLanguage;
      const corrected = await correctChunksBatch(docChunks, docLang, aggressiveCorrection);
      processedChunks.push(...corrected);
    }
  }

  // âœ… Step 9: Group chunks by document and format context
  if (processedChunks.length > 0) {
    const chunksByDocument = new Map<string, any[]>();
    
    processedChunks.forEach(chunk => {
      if (!chunksByDocument.has(chunk.document_id)) {
        chunksByDocument.set(chunk.document_id, []);
      }
      chunksByDocument.get(chunk.document_id)!.push(chunk);
    });

    console.log(`ğŸ“š Chunks distributed across ${chunksByDocument.size} document(s)`);

    const isArabic = responseLanguage === 'ar';
    
    // âœ… Build document-separated context
    const documentContexts = Array.from(chunksByDocument.entries()).map(([docId, docChunks], docIndex) => {
      const docNumber = docIndex + 1;
      const docLang = docLanguages.get(docId);
      const langLabel = docLang === 'ar' ? 'Ø¹Ø±Ø¨ÙŠ' : 'English';
      
      const docHeader = isArabic
        ? `## ğŸ“˜ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© ${docNumber} (${langLabel})`
        : `## ğŸ“˜ Document ${docNumber} (${langLabel})`;
      
      // Group by pages within this document
      const pageGroups = new Map<number, any[]>();
      docChunks.forEach(chunk => {
        if (!pageGroups.has(chunk.page_number)) {
          pageGroups.set(chunk.page_number, []);
        }
        pageGroups.get(chunk.page_number)!.push(chunk);
      });
      
      const pageEntries = Array.from(pageGroups.entries())
        .sort((a, b) => {
          const maxSimA = Math.max(...a[1].map(c => c.similarity || 0));
          const maxSimB = Math.max(...b[1].map(c => c.similarity || 0));
          return maxSimB - maxSimA;
        })
        .slice(0, 10);
      
      const pagesText = pageEntries
        .map(([pageNum, pageChunks]) => {
          const bestSimilarity = Math.max(...pageChunks.map(c => c.similarity || 0));
          const relevanceIcon = bestSimilarity >= 0.5 ? 'ğŸ¯' : bestSimilarity >= 0.4 ? 'âœ“' : 'ğŸ“„';
          const hasCorrected = pageChunks.some(c => c.corrected);
          const correctionBadge = hasCorrected ? ' âœ¨' : '';
          
          const pageHeader = isArabic 
            ? `**${relevanceIcon} ØµÙØ­Ø© ${pageNum}**${correctionBadge}`
            : `**${relevanceIcon} Page ${pageNum}**${correctionBadge}`;
          
          const pageText = pageChunks.map(c => c.chunk_text).join('\n\n');
          return `${pageHeader}\n${pageText}`;
        })
        .join('\n\n---\n\n');
      
      return `${docHeader}\n\n${pagesText}`;
    }).join('\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n');

    const contextTitle = isArabic 
      ? '**ğŸ“š Ù…Ù‚Ø§Ø·Ø¹ Ø°Ø§Øª ØµÙ„Ø© Ù…Ù† Ø§Ù„ÙƒØªØ¨:**'
      : '**ğŸ“š Relevant Passages from the Books:**';

    contextParts.push(`${contextTitle}\n\n${documentContexts}`);

    // âœ… Add multilingual note if applicable
    if (isMultilingual) {
      const multilingualNote = isArabic
        ? '\n\nğŸ“– **Ù…Ù„Ø§Ø­Ø¸Ø©:** Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø© Ù…Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ù„ØºØ§Øª Ù…Ø®ØªÙ„ÙØ© (Ø¹Ø±Ø¨ÙŠ ÙˆØ¥Ù†Ø¬Ù„ÙŠØ²ÙŠ).'
        : '\n\nğŸ“– **Note:** The displayed passages are from documents in different languages (Arabic and English).';
      contextParts.push(multilingualNote);
    }

    // âœ… Add multi-document analysis instruction
    if (documentIds.length > 1 && queryAnalysis.isMultiDocumentQuery) {
      const comparisonInstruction = isArabic
        ? '\n\nâš ï¸ **ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:** Ù‡Ø°Ø§ Ø³Ø¤Ø§Ù„ Ù…Ù‚Ø§Ø±Ù†. Ù‚Ø§Ø±Ù† ÙˆØ­Ù„Ù„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©. Ø£Ø´Ø± Ø¨ÙˆØ¶ÙˆØ­ Ø¥Ù„Ù‰ Ø£ÙˆØ¬Ù‡ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆØ§Ù„Ø§Ø®ØªÙ„Ø§Ù ÙˆØ§Ù„Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„ÙØ±ÙŠØ¯Ø© Ù„ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©.'
        : '\n\nâš ï¸ **Important Instructions:** This is a comparative question. Compare and analyze information from ALL provided documents. Clearly indicate similarities, differences, and unique aspects of each document.';
      contextParts.push(comparisonInstruction);
    }

    // âœ… Add page validation
    const docPageMap = new Map<string, number[]>();
    processedChunks.forEach(chunk => {
      if (!docPageMap.has(chunk.document_id)) {
        docPageMap.set(chunk.document_id, []);
      }
      if (!docPageMap.get(chunk.document_id)!.includes(chunk.page_number)) {
        docPageMap.get(chunk.document_id)!.push(chunk.page_number);
      }
    });
    
    const pageListNote = isArabic
      ? `\n\nâš ï¸ **Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø©:** Ø£Ø¬Ø¨ ÙÙ‚Ø· Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø£Ø¹Ù„Ø§Ù‡. Ù„Ø§ ØªØ°ÙƒØ± Ø£ÙŠ ØµÙØ­Ø§Øª Ø£Ø®Ø±Ù‰.`
      : `\n\nâš ï¸ **Important Note:** Answer only based on the available pages in the context above. Do not reference any other pages.`;
    
    contextParts.push(pageListNote);
  } else {
    console.warn('âš ï¸ No relevant chunks found');
  }

  // âœ… Step 10: Build enhanced prompt with conversation awareness
  const isArabic = responseLanguage === 'ar';
  
  const systemPrompt = isArabic
  ? `Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¨Ø­Ø«ÙŠ Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…ØªØ®ØµØµ ÙŠØªØ°ÙƒØ± Ø§Ù„Ø³ÙŠØ§Ù‚. Ø§Ø³ØªØ®Ø¯Ù… ØªÙ†Ø³ÙŠÙ‚ Markdown ÙÙŠ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ.

ğŸ“‹ **Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:**

1. **Ø§Ù„ÙˆØ¹ÙŠ Ø¨Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©:**
   - **ØªØ°ÙƒØ± Ù…Ø§ Ù†ÙˆÙ‚Ø´ Ø³Ø§Ø¨Ù‚Ø§Ù‹** ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
   - Ø¹Ù†Ø¯ Ø³Ø¤Ø§Ù„Ùƒ Ø¹Ù† Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø³Ø§Ø¨Ù‚Ø©ØŒ Ø§Ø±Ø¬Ø¹ Ø¥Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø£Ø¯Ù†Ø§Ù‡
   - Ø§Ø±Ø¨Ø· Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø¹Ù†Ø¯ Ø§Ù„ØµÙ„Ø©

2. **Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…:**
   - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø£Ø¯Ù†Ø§Ù‡ØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙˆØ£Ø´Ø± Ø¥Ù„Ù‰ Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„ÙˆØ«ÙŠÙ‚Ø©
   - Ø§Ù‚ØªØ¨Ø³ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ø¯Ù‚Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚

3. **Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…Ø© Ø¨Ø«Ù‚Ø©:**
   - **Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø±ÙØªÙƒ Ø§Ù„Ø¹Ø§Ù…Ø© Ø¨Ø­Ø±ÙŠØ©** Ù„ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ÙÙŠØ¯Ø© ÙˆØ´Ø§Ù…Ù„Ø©
   - Ø¹Ù†Ø¯ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø£Ø¯Ø¨ÙŠ Ø£Ùˆ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø§ Ù‡Ùˆ Ù…ØªØ§Ø­ ÙÙŠ Ø§Ù„Ù†Øµ Ø«Ù… Ø£Ø¶Ù Ù…Ù† Ù…Ø¹Ø±ÙØªÙƒ
   - Ø¶Ø¹ Ø¹Ù„Ø§Ù…Ø§Øª ÙˆØ§Ø¶Ø­Ø©:
     * **[Ù…Ù† Ø§Ù„Ù†Øµ - ØµÙØ­Ø© X]** Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚
     * **[Ù…Ù† Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…Ø©]** Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
   - **Ù„Ø§ ØªÙ‚Ù„ "Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ" Ø£Ùˆ "ÙŠØ­ØªØ§Ø¬ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"** - Ù‚Ø¯Ù… Ø£ÙØ¶Ù„ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù…ÙƒÙ†Ø©

4. **Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø«Ù‚Ø©:**
   - Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙ…ÙÙŠØ¯Ø©
   - Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙƒØ§ÙÙŠØ§Ù‹ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø±ÙØªÙƒ Ù„ØªÙƒÙ…Ù„Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
   - **ØªØ¬Ù†Ø¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø§Ø¹ØªØ°Ø§Ø±ÙŠØ© Ø£Ùˆ Ø§Ù„Ù…ØªØ±Ø¯Ø¯Ø©**

5. **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø£Ø¯Ø¨ÙŠ - Ù†Ù‡Ø¬ Ø¹Ù…Ù„ÙŠ:**
   - Ø­Ù„Ù„ Ø§Ù„Ø¹Ù†Ø§ØµØ± Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„Ù†Øµ (Ø§Ù„Ø³Ø±Ø¯ØŒ Ø§Ù„Ù„ØºØ©ØŒ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ØŒ Ø§Ù„Ø£Ø³Ù„ÙˆØ¨)
   - Ù‚Ø§Ø±Ù† Ø¨ÙƒØªÙ‘Ø§Ø¨ Ù…Ø´Ù‡ÙˆØ±ÙŠÙ† Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù†Ø§ØµØ±
   - Ù‚Ø¯Ù… Ø£Ù…Ø«Ù„Ø© Ù…Ø­Ø¯Ø¯Ø© Ù…Ù† Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªØ§Ø­
   - Ø£Ø¶Ù Ù…Ù† Ù…Ø¹Ø±ÙØªÙƒ Ø¹Ù† Ø§Ù„ÙƒØªÙ‘Ø§Ø¨ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡ÙŠÙ†
   - **ÙƒÙ† Ø­Ø§Ø³Ù…Ø§Ù‹ ÙÙŠ Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§ØªÙƒ**

6. **ØªÙ†Ø³ÙŠÙ‚ Markdown:**
   - Ø§Ø³ØªØ®Ø¯Ù… **Ø§Ù„Ù†Øµ Ø§Ù„ØºØ§Ù…Ù‚** Ù„Ù„ØªØ£ÙƒÙŠØ¯
   - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù†Ù‚Ø·ÙŠØ© ÙˆØ§Ù„Ù…Ø±Ù‚Ù…Ø©
   - Ø§Ø³ØªØ®Ø¯Ù… > Ù„Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª Ù…Ù† Ø§Ù„Ù†Øµ

${isMultilingual ? '7. **ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª:** Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ ØªØ±Ø¬Ù…Ù‡Ø§ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©\n' : ''}

${contextualPromptAddition}
${customPrompt ? `\n**ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©:**\n${customPrompt}\n` : ''}`
  : `You are an accurate and specialized research assistant with conversational memory. Use Markdown formatting in your responses.

ğŸ“‹ **Core Guidelines:**

1. **Conversation Awareness:**
   - **Remember what was discussed previously** in this conversation
   - When asked about previous exchanges, refer to the context below
   - Connect new questions to prior topics when relevant

2. **Prioritize Provided Context:**
   - Use passages below and cite page numbers when available
   - Quote information accurately from context

3. **Integrate General Knowledge Confidently:**
   - **Use your general knowledge freely** to provide helpful, comprehensive answers
   - When analyzing literary style or making comparisons, use available text then add from your knowledge
   - Use clear markers:
     * **[From Text - Page X]** for context information
     * **[From General Knowledge]** for external information
   - **Never say "I cannot" or "I need more information"** - provide the best answer possible

4. **Answer ALL Questions Confidently:**
   - Provide direct, helpful answers
   - If context is insufficient, use your knowledge to complete the answer
   - **Avoid apologetic or hesitant responses**

5. **Literary Style Analysis - Practical Approach:**
   - Analyze available elements in text (narrative, language, themes, style)
   - Compare to famous writers based on these elements
   - Provide specific examples from available text
   - Add from your knowledge about similar writers
   - **Be decisive in your conclusions**

6. **Markdown Formatting:**
   - Use **bold** for emphasis
   - Use bullet and numbered lists
   - Use > for quotes from text

${isMultilingual ? '7. **Multilingual:** Passages may contain Arabic text, translate as needed\n' : ''}

${contextualPromptAddition}
${customPrompt ? `\n**Additional Instructions:**\n${customPrompt}\n` : ''}`;

  const userQuery = queryAnalysis?.originalQuery || query;

  const fullPrompt = contextParts.length > 0
    ? `${systemPrompt}${conversationContextString}\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n${contextParts.join('\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n')}\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n**${isArabic ? 'Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…' : "User's Question"}:**\n${userQuery}\n\n**${isArabic ? 'Ø¥Ø¬Ø§Ø¨ØªÙƒ' : 'Your Answer'}:**`
    : `${systemPrompt}${conversationContextString}\n\n**${isArabic ? 'Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…' : "User's Question"}:**\n${userQuery}\n\n**${isArabic ? 'Ø¥Ø¬Ø§Ø¨ØªÙƒ' : 'Your Answer'}:**`;

  console.log('ğŸ¤– Querying Gemini with conversation awareness...');
  console.log(`ğŸ¯ Using model: ${preferredModel || 'default fallback'}`);

  const geminiResult = await generateResponse(fullPrompt, preferredModel);
  const geminiStream = geminiResult.stream;
  const modelUsed = geminiResult.modelUsed;
  
  console.log(`âœ… Response generated using: ${modelUsed}`);
  
  let assistantResponse = '';
  for await (const chunk of geminiStream) {
    const text = chunk.text();
    if (text) {
      assistantResponse += text;
      await writer.write(encoder.encode(text));
    }
  }
  
  // âœ… Save assistant response
  if (sessionId) {
    const messageId = `msg-${Date.now() + 1}-${Math.random().toString(36).substr(2, 9)}`;
    addChatMessage({
      id: messageId,
      sessionId,
      role: 'assistant',
      content: assistantResponse,
      mode: 'reader',
      bookId: undefined,
      bookTitle,
      bookPage
    });
  }
  
  console.log('âœ… Response complete');
}

// ==================== GENERAL CHAT HANDLER ====================
async function handleGeneralChat(
  writer: WritableStreamDefaultWriter,
  encoder: TextEncoder,
  message: string,
  sessionId: string,
  extractedText?: string,
  bookPage?: number,
  history?: Array<{ role: string; content: string }>,
  preferredModel?: string
) {
  // Build conversation context
  let conversationContext = '';
  let contextualPromptAddition = '';
  
  if (history && history.length > 0) {
    const db = getDb();
    const contexts = getSessionContexts(sessionId) as Array<{
      topic: string;
      keywords: string;
    }>;

    if (contexts.length > 0) {
      const recentTopics = contexts.slice(0, 3).map(c => c.topic).join(', ');
      const queryLang = detectQueryLanguage(message);
      contextualPromptAddition = queryLang === 'ar'
        ? `\nğŸ“‹ **Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ù…Ù†Ø§Ù‚Ø´Ø©:** ${recentTopics}\n`
        : `\nğŸ“‹ **Topics discussed:** ${recentTopics}\n`;
    }

    conversationContext = history
      .map(msg => `${msg.role === 'user' ? 'User' : 'Assistant'}: ${msg.content}`)
      .join('\n\n');
  }

  let contextSection = '';
  if (extractedText) {
    contextSection = `\n\n---
**Context from Page ${bookPage || 'current page'}:**
${extractedText}
---`;
  }

  const queryLang = detectQueryLanguage(message);
  const langInstruction = queryLang === 'ar' 
    ? 'Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ†Ø³ÙŠÙ‚ Markdown. ØªØ°ÙƒØ± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©.'
    : 'Respond in English using Markdown formatting. Remember the previous conversation.';

  const prompt = conversationContext
    ? `You are a helpful assistant with conversation memory. ${langInstruction}
${contextualPromptAddition}
${contextSection}

**Previous conversation:**
${conversationContext}

**User:** ${message}
**Assistant:**`
    : `You are a helpful assistant. ${langInstruction}
${contextSection}

**User:** ${message}
**Assistant:**`;

  const geminiResult = await generateResponse(prompt, preferredModel); // âœ… ADD preferredModel
  const geminiStream = geminiResult.stream;
  
  let assistantResponse = '';
  for await (const chunk of geminiStream) {
    const text = chunk.text();
    if (text) {
      assistantResponse += text;
      await writer.write(encoder.encode(text));
    }
  }
  
  // Save assistant response
  const messageId = `msg-${Date.now() + 1}-${Math.random().toString(36).substr(2, 9)}`;
  addChatMessage({
    id: messageId,
    sessionId,
    role: 'assistant',
    content: assistantResponse,
    mode: 'reader',
    bookPage
  });
}

// ==================== SIMPLE QUERY HANDLER ====================
async function handleSimpleQuery(
  writer: WritableStreamDefaultWriter,
  encoder: TextEncoder,
  query: string,
  extractedText?: string,
  preferredModel?: string
) {
  const queryLang = detectQueryLanguage(query);
  const langInstruction = queryLang === 'ar' 
    ? 'Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ†Ø³ÙŠÙ‚ Markdown.'
    : 'Respond in English using Markdown formatting.';

  let contextSection = '';
  if (extractedText) {
    const contextLabel = queryLang === 'ar' ? 'Ø§Ù„Ø³ÙŠØ§Ù‚' : 'Context';
    contextSection = `\n\n---
**${contextLabel}:**
${extractedText}
---`;
  }

  const prompt = `You are a helpful assistant. ${langInstruction}
${contextSection}

**User:** ${query}
**Assistant:**`;

  const geminiResult = await generateResponse(prompt, preferredModel); // âœ… ADD preferredModel
  const geminiStream = geminiResult.stream;
  
  for await (const chunk of geminiStream) {
    const text = chunk.text();
    if (text) {
      await writer.write(encoder.encode(text));
    }
  }
}