import { NextRequest } from 'next/server';
import { generateResponse } from '@/lib/gemini';
import { getDb } from '@/lib/db';
import { analyzeQuery } from '@/lib/queryProcessor';
import { retrieveSmartContext } from '@/lib/smartRetrieval';
import { correctChunksBatch } from '@/lib/spellingCorrection';
import { createClient } from '@supabase/supabase-js';

export const dynamic = 'force-dynamic';
export const runtime = 'nodejs';

const supabaseAdmin = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

/**
 * âœ… Detect document language from Supabase embeddings
 */
async function detectDocumentLanguage(documentId: string): Promise<'ar' | 'en'> {
  try {
    console.log(`ğŸ” Detecting language for document: ${documentId}`);

    const { data, error } = await supabaseAdmin
      .from('embeddings')
      .select('chunk_text')
      .eq('document_id', documentId)
      .limit(20);

    if (error) {
      console.error('âš ï¸ Error fetching embeddings:', error);
      return 'ar';
    }

    if (!data || data.length === 0) {
      console.warn('âš ï¸ No embeddings found for document, defaulting to Arabic');
      return 'ar';
    }

    const contentChunks = data.filter(row => {
      const text = row.chunk_text.toLowerCase();
      return !(
        text.includes('table of contents') ||
        text.includes('chapter') && text.length < 100 ||
        /^page \d+/i.test(text) ||
        text === 'ÙÙŠ Ø±Ø­Ø§Ø¨ Ø£Ù…Ø±ÙŠÙƒØ§'
      );
    });

    const chunksToAnalyze = contentChunks.length > 0 ? contentChunks : data;
    const combinedText = chunksToAnalyze.map(row => row.chunk_text).join(' ');
    
    const arabicChars = (combinedText.match(/[\u0600-\u06FF]/g) || []).length;
    const totalChars = combinedText.replace(/\s/g, '').length;

    const arabicRatio = arabicChars / totalChars;
    const detectedLang = arabicRatio > 0.3 ? 'ar' : 'en';

    console.log(`   âœ… Language detected: ${detectedLang} (${(arabicRatio * 100).toFixed(1)}% Arabic, analyzed ${chunksToAnalyze.length} chunks)`);

    return detectedLang;

  } catch (error) {
    console.error('âŒ Error in detectDocumentLanguage:', error);
    return 'ar';
  }
}

/**
 * âœ… Detect user's query language
 */
function detectQueryLanguage(query: string): 'ar' | 'en' {
  const arabicChars = (query.match(/[\u0600-\u06FF]/g) || []).length;
  const totalChars = query.replace(/\s/g, '').length;
  const arabicRatio = arabicChars / totalChars;
  
  return arabicRatio > 0.3 ? 'ar' : 'en';
}

/**
 * âœ… Detect languages for multiple documents
 */
async function detectMultipleDocumentLanguages(documentIds: string[]): Promise<{
  primary: 'ar' | 'en';
  languages: Map<string, 'ar' | 'en'>;
  isMultilingual: boolean;
}> {
  const languages = new Map<string, 'ar' | 'en'>();
  
  for (const docId of documentIds) {
    const lang = await detectDocumentLanguage(docId);
    languages.set(docId, lang);
  }
  
  const arabicCount = Array.from(languages.values()).filter(l => l === 'ar').length;
  const englishCount = languages.size - arabicCount;
  
  const primary = arabicCount >= englishCount ? 'ar' : 'en';
  const isMultilingual = arabicCount > 0 && englishCount > 0;
  
  console.log(`ğŸŒ Multi-document language analysis:
   - Total documents: ${documentIds.length}
   - Arabic: ${arabicCount}
   - English: ${englishCount}
   - Primary: ${primary}
   - Multilingual: ${isMultilingual}`);
  
  return { primary, languages, isMultilingual };
}

export async function POST(req: NextRequest) {
  const encoder = new TextEncoder();
  const stream = new TransformStream();
  const writer = stream.writable.getWriter();

  (async () => {
    try {
      const { 
        message, 
        query,
        sessionId, 
        bookId, 
        bookTitle, 
        bookPage,
        extractedText,
        documentIds,
        correctSpelling = false,
        aggressiveCorrection = false,
        customPrompt
      } = await req.json();

      const userMessage = message || query;

      console.log('ğŸ“š Reader Chat:', {
        sessionId,
        hasMessage: !!userMessage,
        hasCorpus: documentIds?.length > 0,
        corpusCount: documentIds?.length || 0,
        correctSpelling,
        aggressiveCorrection
      });

      if (!userMessage) {
        await writer.write(encoder.encode('Error: Missing message or query'));
        await writer.close();
        return;
      }

      if (documentIds && documentIds.length > 0) {
        console.log('ğŸ”„ Using corpus retrieval for Reader Chat');
        await handleCorpusQuery(
          writer, 
          encoder, 
          userMessage, 
          documentIds, 
          extractedText, 
          correctSpelling, 
          aggressiveCorrection,
          customPrompt
        );
      } 
      else if (sessionId) {
        console.log('ğŸ’¬ Using general chat with history for Reader Chat');
        await handleGeneralChat(writer, encoder, userMessage, sessionId, extractedText, bookPage);
      }
      else {
        console.log('ğŸ“ Using simple query response');
        await handleSimpleQuery(writer, encoder, userMessage, extractedText);
      }

      await writer.close();

    } catch (error) {
      console.error('âŒ Reader chat error:', error);
      try {
        const errorMsg = error instanceof Error ? error.message : 'Unknown error';
        await writer.write(encoder.encode(`Error: ${errorMsg}`));
        await writer.close();
      } catch {}
    }
  })();

  return new Response(stream.readable, {
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
    },
  });
}

// ==================== CORPUS QUERY HANDLER (UPGRADED) ====================
async function handleCorpusQuery(
  writer: WritableStreamDefaultWriter,
  encoder: TextEncoder,
  query: string,
  documentIds: string[],
  extractedText?: string,
  correctSpelling?: boolean,
  aggressiveCorrection?: boolean,
  customPrompt?: string
) {
  const contextParts: string[] = [];

  // âœ… Step 1: Detect languages for all documents
  const { primary: documentLanguage, languages: docLanguages, isMultilingual } = 
    await detectMultipleDocumentLanguages(documentIds);

  // âœ… Step 2: Detect user's query language
  const queryLanguage = detectQueryLanguage(query);
  console.log(`ğŸ—£ï¸ Query language: ${queryLanguage}`);

  // âœ… Step 3: Determine response language
  const responseLanguage = queryLanguage;
  console.log(`ğŸ’¬ Response will be in: ${responseLanguage}`);

  // âœ… Step 4: Analyze and translate query
  const queryAnalysis = await analyzeQuery(query, documentLanguage);
  console.log('ğŸ” Query Analysis:', {
    original: queryAnalysis.originalQuery,
    translated: queryAnalysis.translatedQuery,
    type: queryAnalysis.queryType,
    keywords: queryAnalysis.keywords,
    isMultiDoc: queryAnalysis.isMultiDocumentQuery
  });

  // âœ… Step 5: Add extracted text if provided
  if (extractedText) {
    const extractLabel = responseLanguage === 'ar' 
      ? '**ğŸ“„ Ù†Øµ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©:**'
      : '**ğŸ“„ Current Page Text:**';
    contextParts.push(`${extractLabel}\n${extractedText}`);
  }

  // âœ… Step 6: Smart corpus retrieval
  console.log('ğŸ”„ Starting smart retrieval...');
  const { chunks, strategy, confidence } = await retrieveSmartContext(queryAnalysis, documentIds);
  
  console.log(`ğŸ“Š Retrieval Results:
   - Strategy: ${strategy}
   - Chunks found: ${chunks.length}
   - Confidence: ${(confidence * 100).toFixed(1)}%`);

  // âœ… Step 7: Process chunks with optional spelling correction
  let processedChunks = chunks;
  if (correctSpelling && chunks.length > 0) {
    console.log('ğŸ”§ Applying spelling correction...');
    
    const chunksByDoc = new Map<string, any[]>();
    chunks.forEach(chunk => {
      const docId = chunk.document_id;
      if (!chunksByDoc.has(docId)) {
        chunksByDoc.set(docId, []);
      }
      chunksByDoc.get(docId)!.push(chunk);
    });

    processedChunks = [];
    for (const [docId, docChunks] of chunksByDoc.entries()) {
      const docLang = docLanguages.get(docId) || documentLanguage;
      const corrected = await correctChunksBatch(docChunks, docLang, aggressiveCorrection);
      processedChunks.push(...corrected);
    }
  }

  // âœ… Step 8: Group chunks by document and format context
  if (processedChunks.length > 0) {
    const chunksByDocument = new Map<string, any[]>();
    
    processedChunks.forEach(chunk => {
      if (!chunksByDocument.has(chunk.document_id)) {
        chunksByDocument.set(chunk.document_id, []);
      }
      chunksByDocument.get(chunk.document_id)!.push(chunk);
    });

    console.log(`ğŸ“š Chunks distributed across ${chunksByDocument.size} document(s)`);

    const isArabic = responseLanguage === 'ar';
    
    // âœ… Build document-separated context
    const documentContexts = Array.from(chunksByDocument.entries()).map(([docId, docChunks], docIndex) => {
      const docNumber = docIndex + 1;
      const docLang = docLanguages.get(docId);
      const langLabel = docLang === 'ar' ? 'Ø¹Ø±Ø¨ÙŠ' : 'English';
      
      const docHeader = isArabic
        ? `## ğŸ“˜ Ø§Ù„ÙˆØ«ÙŠÙ‚Ø© ${docNumber} (${langLabel})`
        : `## ğŸ“˜ Document ${docNumber} (${langLabel})`;
      
      // Group by pages within this document
      const pageGroups = new Map<number, any[]>();
      docChunks.forEach(chunk => {
        if (!pageGroups.has(chunk.page_number)) {
          pageGroups.set(chunk.page_number, []);
        }
        pageGroups.get(chunk.page_number)!.push(chunk);
      });
      
      const pageEntries = Array.from(pageGroups.entries())
        .sort((a, b) => {
          const maxSimA = Math.max(...a[1].map(c => c.similarity || 0));
          const maxSimB = Math.max(...b[1].map(c => c.similarity || 0));
          return maxSimB - maxSimA;
        })
        .slice(0, 10);
      
      const pagesText = pageEntries
        .map(([pageNum, pageChunks]) => {
          const bestSimilarity = Math.max(...pageChunks.map(c => c.similarity || 0));
          const relevanceIcon = bestSimilarity >= 0.5 ? 'ğŸ¯' : bestSimilarity >= 0.4 ? 'âœ“' : 'ğŸ“„';
          const hasCorrected = pageChunks.some(c => c.corrected);
          const correctionBadge = hasCorrected ? ' âœ¨' : '';
          
          const pageHeader = isArabic 
            ? `**${relevanceIcon} ØµÙØ­Ø© ${pageNum}**${correctionBadge}`
            : `**${relevanceIcon} Page ${pageNum}**${correctionBadge}`;
          
          const pageText = pageChunks.map(c => c.chunk_text).join('\n\n');
          return `${pageHeader}\n${pageText}`;
        })
        .join('\n\n---\n\n');
      
      return `${docHeader}\n\n${pagesText}`;
    }).join('\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n');

    const contextTitle = isArabic 
      ? '**ğŸ“š Ù…Ù‚Ø§Ø·Ø¹ Ø°Ø§Øª ØµÙ„Ø© Ù…Ù† Ø§Ù„ÙƒØªØ¨:**'
      : '**ğŸ“š Relevant Passages from the Books:**';

    contextParts.push(`${contextTitle}\n\n${documentContexts}`);

    // âœ… Add multilingual note if applicable
    if (isMultilingual) {
      const multilingualNote = isArabic
        ? '\n\nğŸ“– **Ù…Ù„Ø§Ø­Ø¸Ø©:** Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø© Ù…Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ù„ØºØ§Øª Ù…Ø®ØªÙ„ÙØ© (Ø¹Ø±Ø¨ÙŠ ÙˆØ¥Ù†Ø¬Ù„ÙŠØ²ÙŠ).'
        : '\n\nğŸ“– **Note:** The displayed passages are from documents in different languages (Arabic and English).';
      contextParts.push(multilingualNote);
    }

    // âœ… Add multi-document analysis instruction
    if (documentIds.length > 1 && queryAnalysis.isMultiDocumentQuery) {
      const comparisonInstruction = isArabic
        ? '\n\nâš ï¸ **ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:** Ù‡Ø°Ø§ Ø³Ø¤Ø§Ù„ Ù…Ù‚Ø§Ø±Ù†. Ù‚Ø§Ø±Ù† ÙˆØ­Ù„Ù„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©. Ø£Ø´Ø± Ø¨ÙˆØ¶ÙˆØ­ Ø¥Ù„Ù‰ Ø£ÙˆØ¬Ù‡ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆØ§Ù„Ø§Ø®ØªÙ„Ø§Ù ÙˆØ§Ù„Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„ÙØ±ÙŠØ¯Ø© Ù„ÙƒÙ„ ÙˆØ«ÙŠÙ‚Ø©.'
        : '\n\nâš ï¸ **Important Instructions:** This is a comparative question. Compare and analyze information from ALL provided documents. Clearly indicate similarities, differences, and unique aspects of each document.';
      contextParts.push(comparisonInstruction);
    }

    // âœ… Add page validation
    const docPageMap = new Map<string, number[]>();
    processedChunks.forEach(chunk => {
      if (!docPageMap.has(chunk.document_id)) {
        docPageMap.set(chunk.document_id, []);
      }
      if (!docPageMap.get(chunk.document_id)!.includes(chunk.page_number)) {
        docPageMap.get(chunk.document_id)!.push(chunk.page_number);
      }
    });
    
    const pageListNote = isArabic
      ? `\n\nâš ï¸ **Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø©:** Ø£Ø¬Ø¨ ÙÙ‚Ø· Ø§Ø³ØªÙ†Ø§Ø¯Ù‹Ø§ Ø¥Ù„Ù‰ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø£Ø¹Ù„Ø§Ù‡. Ù„Ø§ ØªØ°ÙƒØ± Ø£ÙŠ ØµÙØ­Ø§Øª Ø£Ø®Ø±Ù‰.`
      : `\n\nâš ï¸ **Important Note:** Answer only based on the available pages in the context above. Do not reference any other pages.`;
    
    contextParts.push(pageListNote);
  } else {
    console.warn('âš ï¸ No relevant chunks found');
  }

  // âœ… Step 9: Build enhanced prompt
  const isArabic = responseLanguage === 'ar';
  
  const systemPrompt = isArabic
    ? `Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø¨Ø­Ø«ÙŠ Ø¯Ù‚ÙŠÙ‚ ÙˆÙ…ØªØ®ØµØµ. Ø§Ø³ØªØ®Ø¯Ù… ØªÙ†Ø³ÙŠÙ‚ Markdown ÙÙŠ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ.

ğŸ“‹ **Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:**

1. **Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…:**
   - Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø£Ø¯Ù†Ø§Ù‡ØŒ Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙˆØ£Ø´Ø± Ø¥Ù„Ù‰ Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„ÙˆØ«ÙŠÙ‚Ø©
   - Ø§Ù‚ØªØ¨Ø³ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ø¯Ù‚Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚

2. **Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…Ø©:**
   - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ Ù†Ø§Ù‚ØµÙ‹Ø§ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù…Ø¹Ø±ÙØªÙƒ
   - **ÙˆØ¶Ù‘Ø­ Ø¨ÙˆØ¶ÙˆØ­** Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚

3. **Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„Ø©:**
   - Ø§Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ ÙˆØ§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…Ø©
   - Ø±ØªØ¨ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ ÙˆÙ…Ù†Ø¸Ù…
   - Ø§Ø³ØªØ®Ø¯Ù… Ø£Ù‚Ø³Ø§Ù… ÙˆØ§Ø¶Ø­Ø©:
     * **[Ù…Ù† Ø§Ù„Ù†Øµ]** Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚
     * **[Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©]** Ù„Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø¹Ø§Ù…Ø©

4. **ØªÙ†Ø³ÙŠÙ‚ Markdown:**
   - Ø§Ø³ØªØ®Ø¯Ù… **Ø§Ù„Ù†Øµ Ø§Ù„ØºØ§Ù…Ù‚** Ù„Ù„ØªØ£ÙƒÙŠØ¯
   - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù†Ù‚Ø·ÙŠØ© ÙˆØ§Ù„Ù…Ø±Ù‚Ù…Ø©
   - Ø§Ø³ØªØ®Ø¯Ù… > Ù„Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª

${isMultilingual ? '5. **ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª:** Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ ØªØ±Ø¬Ù…Ù‡Ø§ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø§Ø¬Ø©\n' : ''}

${customPrompt ? `\n**ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©:**\n${customPrompt}\n` : ''}`
    : `You are an accurate and specialized research assistant. Use Markdown formatting in your responses.

ğŸ“‹ **Core Guidelines:**

1. **Prioritize Provided Context:**
   - Use passages below and cite page numbers and document numbers
   - Quote information accurately

2. **Integrate General Knowledge:**
   - Add general knowledge if context is limited
   - **Clearly indicate** information NOT from context

3. **Comprehensive Answers:**
   - Combine context with general knowledge
   - Use clear sections:
     * **[From Text]** for context information
     * **[Additional Information]** for general knowledge

4. **Markdown Formatting:**
   - Use **bold**, lists, > for quotes

${isMultilingual ? '5. **Multilingual:** Passages may contain Arabic text, translate as needed\n' : ''}

${customPrompt ? `\n**Additional Instructions:**\n${customPrompt}\n` : ''}`;

  const userQuery = queryAnalysis?.originalQuery || query;

  const fullPrompt = contextParts.length > 0
    ? `${systemPrompt}\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n${contextParts.join('\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n')}\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n**${isArabic ? 'Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…' : "User's Question"}:**\n${userQuery}\n\n**${isArabic ? 'Ø¥Ø¬Ø§Ø¨ØªÙƒ' : 'Your Answer'}:**`
    : `${systemPrompt}\n\n**${isArabic ? 'Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…' : "User's Question"}:**\n${userQuery}\n\n**${isArabic ? 'Ø¥Ø¬Ø§Ø¨ØªÙƒ' : 'Your Answer'}:**`;

  console.log('ğŸ¤– Querying Gemini...');
  const geminiStream = await generateResponse(fullPrompt);
  for await (const chunk of geminiStream) {
    const text = chunk.text();
    if (text) await writer.write(encoder.encode(text));
  }
  console.log('âœ… Response complete');
}

// ==================== GENERAL CHAT HANDLER ====================
async function handleGeneralChat(
  writer: WritableStreamDefaultWriter,
  encoder: TextEncoder,
  message: string,
  sessionId: string,
  extractedText?: string,
  bookPage?: number
) {
  const db = getDb();
  const history = db.prepare(`
    SELECT role, content 
    FROM chat_messages 
    WHERE session_id = ? 
    ORDER BY created_at ASC
  `).all(sessionId) as Array<{ role: string; content: string }>;

  let conversationContext = '';
  if (history.length > 0) {
    conversationContext = history
      .map(msg => `${msg.role === 'user' ? 'User' : 'Assistant'}: ${msg.content}`)
      .join('\n\n');
  }

  let contextSection = '';
  if (extractedText) {
    contextSection = `\n\n---
**Context from Page ${bookPage || 'current page'}:**
${extractedText}
---`;
  }

  const queryLang = detectQueryLanguage(message);
  const langInstruction = queryLang === 'ar' 
    ? 'Respond in Arabic using proper Markdown formatting.'
    : 'Respond in English using proper Markdown formatting.';

  const prompt = conversationContext
    ? `You are a helpful assistant. ${langInstruction}
${contextSection}

**Previous conversation:**
${conversationContext}

**User:** ${message}
**Assistant:**`
    : `You are a helpful assistant. ${langInstruction}
${contextSection}

**User:** ${message}
**Assistant:**`;

  const geminiStream = await generateResponse(prompt);
  for await (const chunk of geminiStream) {
    const text = chunk.text();
    if (text) {
      await writer.write(encoder.encode(text));
    }
  }
}

// ==================== SIMPLE QUERY HANDLER ====================
async function handleSimpleQuery(
  writer: WritableStreamDefaultWriter,
  encoder: TextEncoder,
  query: string,
  extractedText?: string
) {
  const queryLang = detectQueryLanguage(query);
  const langInstruction = queryLang === 'ar' 
    ? 'Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ†Ø³ÙŠÙ‚ Markdown.'
    : 'Respond in English using Markdown formatting.';

  let contextSection = '';
  if (extractedText) {
    const contextLabel = queryLang === 'ar' ? 'Ø§Ù„Ø³ÙŠØ§Ù‚' : 'Context';
    contextSection = `\n\n---
**${contextLabel}:**
${extractedText}
---`;
  }

  const prompt = `You are a helpful assistant. ${langInstruction}
${contextSection}

**User:** ${query}
**Assistant:**`;

  const geminiStream = await generateResponse(prompt);
  for await (const chunk of geminiStream) {
    const text = chunk.text();
    if (text) {
      await writer.write(encoder.encode(text));
    }
  }
}