import { NextRequest } from 'next/server';
import { generateResponse } from '@/lib/gemini';
import { analyzeQuery } from '@/lib/queryProcessor';
import { retrieveSmartContext } from '@/lib/smartRetrieval';
import { correctChunksBatch } from '@/lib/spellingCorrection';
import { getDb } from '@/lib/db';
import { createClient } from '@supabase/supabase-js';

export const dynamic = 'force-dynamic';
export const runtime = 'nodejs';

// âœ… Initialize Supabase client
const supabaseAdmin = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

export async function POST(request: NextRequest) {
  const encoder = new TextEncoder();
  const stream = new TransformStream();
  const writer = stream.writable.getWriter();

  (async () => {
    try {
      const { 
        query, 
        documentIds, 
        extractedText, 
        customPrompt,
        correctSpelling = false,
        aggressiveCorrection = false
      } = await request.json();

      console.log('ğŸ“ Reader Query:', {
        hasQuery: !!query,
        hasExtractedText: !!extractedText,
        hasCustomPrompt: !!customPrompt,
        documentIds: documentIds?.length || 0,
        correctSpelling,
        aggressiveCorrection
      });

      if (!query && !extractedText) {
        await writer.write(encoder.encode('Please provide a query or extracted text.'));
        await writer.close();
        return;
      }

      const contextParts: string[] = [];

      // âœ… Step 1: Detect document language
      const documentLanguage = documentIds && documentIds.length > 0
        ? await detectDocumentLanguage(documentIds[0])
        : 'ar';

      console.log(`ğŸ“– Document language: ${documentLanguage}`);

      // âœ… Step 2: Analyze and translate query
      let queryAnalysis: any = null;
      if (query) {
        queryAnalysis = await analyzeQuery(query, documentLanguage);
        console.log('ğŸ” Query Analysis:', {
          original: queryAnalysis.originalQuery,
          translated: queryAnalysis.translatedQuery,
          type: queryAnalysis.queryType,
          keywords: queryAnalysis.keywords
        });
      }

      // âœ… Step 3: Add extracted text
      if (extractedText) {
        const extractLabel = documentLanguage === 'ar' 
          ? '**ğŸ“„ Ù†Øµ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©:**'
          : '**ğŸ“„ Current Page Text:**';
        contextParts.push(`${extractLabel}\n${extractedText}`);
      }

      // âœ… Step 4: Smart corpus retrieval
      if (documentIds && documentIds.length > 0 && queryAnalysis) {
        console.log('ğŸ”„ Starting smart retrieval...');
        
        const { chunks, strategy, confidence } = await retrieveSmartContext(
          queryAnalysis,
          documentIds
        );

        console.log(`ğŸ“Š Retrieval Results:
   - Strategy: ${strategy}
   - Chunks found: ${chunks.length}
   - Confidence: ${(confidence * 100).toFixed(1)}%`);

        if (chunks.length > 0) {
          // âœ… Optional: Correct spelling in retrieved chunks
          let processedChunks = chunks;
          if (correctSpelling) {
            console.log('ğŸ”§ Applying spelling correction...');
            processedChunks = await correctChunksBatch(
              chunks,
              documentLanguage,
              aggressiveCorrection
            );
          }

          // âœ… Group chunks by page number
          const chunksByPage = new Map<number, any[]>();
          processedChunks.slice(0, 30).forEach((chunk: any) => {
            const page = chunk.page_number;
            if (!chunksByPage.has(page)) {
              chunksByPage.set(page, []);
            }
            chunksByPage.get(page)!.push(chunk);
          });

          // âœ… Format grouped chunks with page numbers only
          const isArabic = documentLanguage === 'ar';
          const pageEntries = Array.from(chunksByPage.entries())
            .sort((a, b) => {
              const maxSimA = Math.max(...a[1].map(c => c.similarity || 0));
              const maxSimB = Math.max(...b[1].map(c => c.similarity || 0));
              return maxSimB - maxSimA;
            })
            .slice(0, 15); // Limit to 15 pages max

          const corpusContext = pageEntries
            .map(([pageNum, pageChunks]) => {
              const bestSimilarity = Math.max(...pageChunks.map(c => c.similarity || 0));
              
              // Relevance indicators
              const relevanceIcon = bestSimilarity >= 0.5 
                ? 'ğŸ¯' 
                : bestSimilarity >= 0.4 
                  ? 'âœ“' 
                  : 'ğŸ“„';
              
              const hasCorrected = pageChunks.some(c => c.corrected);
              const correctionBadge = hasCorrected ? ' âœ¨' : '';
              
              // Page header
              const pageHeader = isArabic 
                ? `**${relevanceIcon} ØµÙØ­Ø© ${pageNum}**${correctionBadge}`
                : `**${relevanceIcon} Page ${pageNum}**${correctionBadge}`;
              
              // Combine all chunks from this page
              const pageText = pageChunks
                .map(c => c.chunk_text)
                .join('\n\n');
              
              return `${pageHeader}\n${pageText}`;
            })
            .join('\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n');

          const contextTitle = isArabic 
            ? '**ğŸ“š Ù…Ù‚Ø§Ø·Ø¹ Ø°Ø§Øª ØµÙ„Ø© Ù…Ù† Ø§Ù„ÙƒØªØ§Ø¨:**'
            : '**ğŸ“š Relevant Passages from the Book:**';

          contextParts.push(`${contextTitle}\n\n${corpusContext}`);
        } else {
          console.warn('âš ï¸ No relevant chunks found');
        }
      }

      // âœ… Step 5: Build enhanced prompt with citation instructions
      const isArabic = documentLanguage === 'ar';
      
      const systemPrompt = isArabic
        ? `Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø¨ÙŠ ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠ.

ğŸ“‹ Ù…Ù‡Ù…ØªÙƒ:
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù… Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø¯Ù‚Ø© ÙˆØ¹Ù…Ù‚
- Ø§Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ø·Ø¹ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
- **Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø©ØŒ Ø§Ø°ÙƒØ± Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© ÙÙ‚Ø·** (Ù…Ø«Ø§Ù„: "Ø­Ø³Ø¨ Ù…Ø§ ÙˆØ±Ø¯ ÙÙŠ ØµÙØ­Ø© 15..." Ø£Ùˆ "(ØµÙØ­Ø© 15)")
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ©ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­
- Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙ…Ù†Ø¸Ù…Ø© ÙˆÙ…ØªØ±Ø§Ø¨Ø·Ø©
- **Ù„Ø§ ØªØ°ÙƒØ± "Chunk" Ø£Ùˆ "Ù…Ù‚Ø·Ø¹ Ø±Ù‚Ù…" Ø£Ùˆ Ø£ÙŠ Ù…ØµØ·Ù„Ø­Ø§Øª ØªÙ‚Ù†ÙŠØ©**
- Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ³Ù„Ø³Ø©

${customPrompt ? `\n**ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©:**\n${customPrompt}\n` : ''}`
        : `You are an intelligent assistant specialized in literary analysis and Islamic research.

ğŸ“‹ Your Task:
- Use the provided context to answer accurately and deeply
- Synthesize information from all relevant passages
- **When citing information, only mention page numbers** (example: "As stated on page 15..." or "(page 15)")
- If information is insufficient, state it clearly
- Provide a comprehensive, organized, and coherent answer
- **Do NOT mention "Chunk" or any technical terms**
- Make the answer natural and flowing

${customPrompt ? `\n**Additional Instructions:**\n${customPrompt}\n` : ''}`;

      const userQuery = queryAnalysis?.originalQuery || query || 'Please analyze the extracted text.';

      const fullPrompt = contextParts.length > 0
        ? `${systemPrompt}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

${contextParts.join('\n\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n')}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**${isArabic ? 'Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…' : "User's Question"}:**
${userQuery}

**${isArabic ? 'Ø¥Ø¬Ø§Ø¨ØªÙƒ' : 'Your Answer'}:**`
        : `${systemPrompt}

**${isArabic ? 'Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…' : "User's Question"}:**
${userQuery}

**${isArabic ? 'Ø¥Ø¬Ø§Ø¨ØªÙƒ' : 'Your Answer'}:**`;

      console.log('ğŸ¤– Querying Gemini...');

      // âœ… Stream response
      const geminiStream = await generateResponse(fullPrompt);

      for await (const chunk of geminiStream) {
        const text = chunk.text();
        if (text) {
          await writer.write(encoder.encode(text));
        }
      }

      console.log('âœ… Response complete');
      await writer.close();

    } catch (error) {
      console.error('âŒ Reader query error:', error);
      
      try {
        const errorMsg = error instanceof Error ? error.message : 'Unknown error';
        await writer.write(encoder.encode(`Error: ${errorMsg}`));
        await writer.close();
      } catch {}
    }
  })();

  return new Response(stream.readable, {
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
    },
  });
}

/**
 * âœ… Detect document language from Supabase embeddings
 */
async function detectDocumentLanguage(documentId: string): Promise<'ar' | 'en'> {
  try {
    console.log(`ğŸ” Detecting language for document: ${documentId}`);

    // âœ… Query Supabase embeddings table
    const { data, error } = await supabaseAdmin
      .from('embeddings')
      .select('chunk_text')
      .eq('document_id', documentId)
      .limit(5);

    if (error) {
      console.error('âš ï¸ Error fetching embeddings:', error);
      return 'ar'; // Default to Arabic
    }

    if (!data || data.length === 0) {
      console.warn('âš ï¸ No embeddings found for document, defaulting to Arabic');
      return 'ar';
    }

    // Analyze language from sample chunks
    const combinedText = data.map(row => row.chunk_text).join(' ');
    const arabicChars = (combinedText.match(/[\u0600-\u06FF]/g) || []).length;
    const totalChars = combinedText.replace(/\s/g, '').length;

    const arabicRatio = arabicChars / totalChars;
    const detectedLang = arabicRatio > 0.5 ? 'ar' : 'en';

    console.log(`   âœ… Language detected: ${detectedLang} (${(arabicRatio * 100).toFixed(1)}% Arabic)`);

    return detectedLang;

  } catch (error) {
    console.error('âŒ Error in detectDocumentLanguage:', error);
    return 'ar'; // Safe default
  }
}