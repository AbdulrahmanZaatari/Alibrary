import { NextRequest } from 'next/server';
import { generateResponse } from '@/lib/gemini';
import { 
  getDb, 
  addChatMessage, 
  updateChatSessionTimestamp,
  trackConversationContext,
  createSessionSummary,
  trackGlobalMemory,
  getSessionContexts
} from '@/lib/db';
import { analyzeQuery } from '@/lib/queryProcessor';
import { retrieveSmartContext, detectFollowUpWithAI } from '@/lib/smartRetrieval';
import { 
  isComplexQuery, 
  performMultiHopReasoning, 
  formatMultiHopResponse 
} from '@/lib/multiHopReasoning';
import { 
  analyzeConversationContext, 
  generateSessionSummary, 
  extractTopicsFromMessage 
} from '@/lib/contextAnalyzer';

export const dynamic = 'force-dynamic';
export const runtime = 'nodejs';

/**
 * ‚úÖ Detect query language
 */
function detectQueryLanguage(query: string): 'ar' | 'en' {
  const arabicChars = (query.match(/[\u0600-\u06FF]/g) || []).length;
  const totalChars = query.replace(/\s/g, '').length;
  const arabicRatio = arabicChars / totalChars;
  
  return arabicRatio > 0.3 ? 'ar' : 'en';
}

export async function POST(request: NextRequest) {
  const encoder = new TextEncoder();
  const stream = new TransformStream();
  const writer = stream.writable.getWriter();

  (async () => {
    try {
      const { 
        message, 
        sessionId, 
        documentIds,
        enableMultiHop = false,
        preferredModel,
        useReranking = true,
        useKeywordSearch = false
      } = await request.json();

      if (!message || !sessionId) {
        await writer.write(encoder.encode('Error: Missing message or sessionId'));
        await writer.close();
        return;
      }

      console.log('üí¨ General Chat:', {
        sessionId,
        hasMessage: !!message,
        hasDocuments: documentIds?.length > 0,
        documentCount: documentIds?.length || 0,
        enableMultiHop,
        preferredModel,
        useKeywordSearch
      });

      const db = getDb();
      
      // ‚úÖ STEP 1: Fetch conversation history
      const history = db.prepare(`
        SELECT role, content, created_at
        FROM chat_messages 
        WHERE session_id = ? 
        ORDER BY created_at DESC
        LIMIT 10
      `).all(sessionId) as Array<{ role: string; content: string; created_at: string }>;

      history.reverse();
      console.log(`üìú Loaded ${history.length} previous messages`);

      // ‚úÖ STEP 1.5: AI-POWERED FOLLOW-UP DETECTION
      const conversationHistory = history.map(msg => ({
        role: msg.role,
        content: msg.content
      }));

      const followUpDetection = await detectFollowUpWithAI(message, conversationHistory);

      console.log(`üîç Follow-up Analysis:`, {
        isFollowUp: followUpDetection.isFollowUp,
        confidence: followUpDetection.confidence,
        reason: followUpDetection.reason,
        needsRetrieval: followUpDetection.needsNewRetrieval
      });

      // ‚úÖ STEP 2: Analyze conversation context (every 3 messages)
      if (history.length > 0 && history.length % 3 === 0) {
        console.log('üß† Analyzing conversation context...');
        
        const queryLanguage = detectQueryLanguage(message);

        try {
          const context = await analyzeConversationContext(conversationHistory, queryLanguage);
          
          if (context.topics.length > 0) {
            for (const topic of context.topics.slice(0, 3)) {
              const contextId = `ctx-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
              trackConversationContext({
                id: contextId,
                sessionId,
                topic,
                keywords: context.keywords,
                entities: context.entities,
                relevanceScore: 0.8
              });
            }
          }

          if (context.mainTheme) {
            const memoryId = `mem-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
            trackGlobalMemory({
              id: memoryId,
              topic: context.mainTheme,
              context: `Intent: ${context.userIntent}, Topics: ${context.topics.join(', ')}`,
              sessionId
            });
          }

          console.log('‚úÖ Context tracked:', {
            topics: context.topics,
            intent: context.userIntent,
            mainTheme: context.mainTheme
          });
        } catch (error) {
          console.error('‚ö†Ô∏è Context analysis failed:', error);
        }
      }

      // ‚úÖ STEP 3: Generate summary (every 10 messages)
      if (history.length > 0 && history.length % 10 === 0) {
        console.log('üìù Generating session summary...');
        
        try {
          const queryLanguage = detectQueryLanguage(message);

          const summaryResult = await generateSessionSummary(conversationHistory, queryLanguage);
          
          const summaryId = `sum-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
          createSessionSummary({
            id: summaryId,
            sessionId,
            summary: summaryResult.summary,
            keyPoints: summaryResult.keyPoints,
            messageCount: history.length
          });

          console.log('‚úÖ Session summary created');
        } catch (error) {
          console.error('‚ö†Ô∏è Summary generation failed:', error);
        }
      }

      // ‚úÖ STEP 4: Build context-aware conversation string
      let conversationContextString = '';
      let contextualPromptAddition = '';
      
      if (history.length > 0) {
        const contexts = getSessionContexts(sessionId) as Array<{
          topic: string;
          keywords: string;
          mention_count: number;
        }>;

        if (contexts.length > 0) {
          const recentTopics = contexts
            .slice(0, 3)
            .map(c => c.topic)
            .join(', ');
          
          const queryLanguage = detectQueryLanguage(message);
          contextualPromptAddition = queryLanguage === 'ar'
            ? `\n\nüìã **ÿßŸÑŸàÿπŸä ÿ®ÿßŸÑÿ≥ŸäÿßŸÇ:**\nÿßŸÑŸÖŸàÿßÿ∂Ÿäÿπ ÿßŸÑÿ™Ÿä ŸÜÿßŸÇÿ¥ŸÜÿßŸáÿß ŸÖÿ§ÿÆÿ±ÿßŸã: ${recentTopics}\n`
            : `\n\nüìã **Context Awareness:**\nRecent topics we've discussed: ${recentTopics}\n`;
        }

        conversationContextString = history
          .map(msg => `${msg.role === 'user' ? 'User' : 'Assistant'}: ${msg.content}`)
          .join('\n\n');
      }

      // ‚úÖ STEP 5: Detect query language and check for complex queries
      const queryLanguage = detectQueryLanguage(message);
      const requiresMultiHop = enableMultiHop && 
                               documentIds?.length > 0 && 
                               isComplexQuery(message);

      // ==================== MULTI-HOP REASONING PATH ====================
      if (requiresMultiHop) {
        console.log('üß† Complex conversational query - activating multi-hop reasoning');
        
        try {
          const docLanguages = new Map<string, 'ar' | 'en'>();
          documentIds.forEach((docId: string) => {
            docLanguages.set(docId, queryLanguage);
          });

          const multiHopResult = await performMultiHopReasoning(
            message,
            documentIds,
            docLanguages,
            3,
            queryLanguage,
            useReranking,
            useKeywordSearch
          );

          let conversationPrefix = '';
          if (history.length > 0) {
            const recentHistory = history.slice(-3);
            conversationPrefix = queryLanguage === 'ar'
              ? `üí≠ **ÿßÿ≥ÿ™ŸÉŸÖÿßŸÑÿßŸã ŸÑŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ©:**\n\n`
              : `üí≠ **Continuing our conversation:**\n\n`;
            
            recentHistory.forEach(msg => {
              const label = msg.role === 'user' 
                ? (queryLanguage === 'ar' ? 'ÿ£ŸÜÿ™' : 'You')
                : (queryLanguage === 'ar' ? 'ÿßŸÑŸÖÿ≥ÿßÿπÿØ' : 'Assistant');
              conversationPrefix += `**${label}:** ${msg.content.substring(0, 100)}${msg.content.length > 100 ? '...' : ''}\n\n`;
            });
            
            conversationPrefix += '---\n\n';
          }

          const formattedResponse = conversationPrefix + formatMultiHopResponse(multiHopResult, queryLanguage);
          await writer.write(encoder.encode(formattedResponse));
          
          console.log('‚úÖ Multi-hop conversational response complete');
          await writer.close();
          return;

        } catch (error) {
          console.error('‚ùå Multi-hop reasoning failed, falling back to standard:', error);
        }
      }

      // ==================== DOCUMENT-BASED RETRIEVAL (IF DOCUMENTS PROVIDED) ====================
      if (documentIds && documentIds.length > 0) {
        console.log('üìö Documents provided - performing retrieval-based chat');
        
        // ‚úÖ Perform query analysis
        const queryAnalysis = await analyzeQuery(message, queryLanguage);
        
        // ‚úÖ ADD follow-up info to query analysis
        queryAnalysis.isFollowUp = followUpDetection.isFollowUp;
        queryAnalysis.followUpConfidence = followUpDetection.confidence;
        queryAnalysis.needsNewRetrieval = followUpDetection.needsNewRetrieval;

        console.log('üîç Query Analysis:', {
          original: queryAnalysis.originalQuery,
          type: queryAnalysis.queryType,
          keywords: queryAnalysis.keywords,
          isFollowUp: queryAnalysis.isFollowUp,
          needsRetrieval: queryAnalysis.needsNewRetrieval
        });

        // ‚úÖ SMART RETRIEVAL DECISION
        let retrievedContext = '';
        
        if (followUpDetection.needsNewRetrieval || !followUpDetection.isFollowUp) {
          console.log('üìö Performing new retrieval...');
          
          const { chunks, strategy, confidence } = await retrieveSmartContext(
            queryAnalysis,
            documentIds,
            useReranking,
            useKeywordSearch
          );
          
          console.log(`üìä Retrieval Results:
   - Strategy: ${strategy}
   - Chunks: ${chunks.length}
   - Confidence: ${(confidence * 100).toFixed(1)}%`);

          if (chunks.length > 0) {
            retrievedContext = chunks
              .map((chunk, i) => {
                const pageHeader = queryLanguage === 'ar'
                  ? `**üìÑ ÿµŸÅÿ≠ÿ© ${chunk.page_number}**`
                  : `**üìÑ Page ${chunk.page_number}**`;
                return `${pageHeader}\n${chunk.chunk_text}`;
              })
              .join('\n\n---\n\n');
          }
        } else {
          console.log('üí¨ Follow-up detected - reusing conversation context');
          
          // Use last 2 assistant messages as context
          retrievedContext = history
            .filter(msg => msg.role === 'assistant')
            .slice(-2)
            .map(msg => msg.content)
            .join('\n\n---\n\n');
        }

        // Build prompt with retrieved context
        const contextSection = retrievedContext
          ? `\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n**${queryLanguage === 'ar' ? 'ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖÿ≥ÿ™ÿ±ÿ¨ÿπ' : 'Retrieved Context'}:**\n\n${retrievedContext}\n\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n\n`
          : '';

        const systemPrompt = queryLanguage === 'ar'
          ? `ÿ£ŸÜÿ™ ŸÖÿ≥ÿßÿπÿØ ÿ®ÿ≠ÿ´Ÿä ÿØŸÇŸäŸÇ Ÿäÿ™ÿ∞ŸÉÿ± ÿßŸÑÿ≥ŸäÿßŸÇ. ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿ™ŸÜÿ≥ŸäŸÇ Markdown.

üìã **ÿßŸÑŸÇŸàÿßÿπÿØ:**
1. ÿ™ÿ∞ŸÉÿ± ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ© ÿßŸÑÿ≥ÿßÿ®ŸÇÿ©
2. ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿ≥ŸäÿßŸÇ ÿßŸÑŸÖŸÇÿØŸÖ ÿπŸÜÿØ ÿ™ŸàŸÅÿ±Ÿá
3. ÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿπÿ±ŸÅÿ™ŸÉ ÿßŸÑÿπÿßŸÖÿ© ÿ®ÿ´ŸÇÿ©
4. ÿ£ÿ¨ÿ® ÿ®ÿ¥ŸÉŸÑ ŸÖÿ®ÿßÿ¥ÿ± ŸàŸÖŸÅŸäÿØ

${contextualPromptAddition}`
          : `You are an accurate research assistant with conversation memory. Use Markdown formatting.

üìã **Guidelines:**
1. Remember previous conversation
2. Use provided context when available
3. Use your general knowledge confidently
4. Answer directly and helpfully

${contextualPromptAddition}`;

        const prompt = conversationContextString
          ? `${systemPrompt}

**Previous conversation:**
${conversationContextString}

${contextSection}

**User:** ${message}
**Assistant:**`
          : `${systemPrompt}

${contextSection}

**User:** ${message}
**Assistant:**`;

        const geminiResult = await generateResponse(prompt, preferredModel);
        const geminiStream = geminiResult.stream;
        const modelUsed = geminiResult.modelUsed;
        
        console.log(`‚úÖ Response generated using: ${modelUsed}`);
        
        for await (const chunk of geminiStream) {
          const text = chunk.text();
          if (text) {
            await writer.write(encoder.encode(text));
          }
        }

        updateChatSessionTimestamp(sessionId);
        await writer.close();
        console.log('‚úÖ Document-based chat response complete');
        return;
      }

      // ==================== STANDARD CONVERSATIONAL CHAT (NO DOCUMENTS) ====================
      console.log(enableMultiHop ? 'üí¨ Using standard conversational response (fallback)' : 'üí¨ Using standard conversational response');

      const systemPrompt = queryLanguage === 'ar'
        ? `ÿ£ŸÜÿ™ ŸÖÿ≥ÿßÿπÿØ ÿ®ÿ≠ÿ´Ÿä ÿØŸÇŸäŸÇ ŸàŸÖÿ™ÿÆÿµÿµ Ÿäÿ™ÿ∞ŸÉÿ± ÿßŸÑÿ≥ŸäÿßŸÇ. ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿ™ŸÜÿ≥ŸäŸÇ Markdown ŸÅŸä ÿ•ÿ¨ÿßÿ®ÿßÿ™ŸÉ.

üìã **ÿßŸÑŸÇŸàÿßÿπÿØ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©:**

1. **ÿßŸÑŸàÿπŸä ÿ®ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©:**
   - **ÿ™ÿ∞ŸÉÿ± ŸÖÿß ŸÜŸàŸÇÿ¥ ÿ≥ÿßÿ®ŸÇÿßŸã** ŸÅŸä Ÿáÿ∞Ÿá ÿßŸÑŸÖÿ≠ÿßÿØÿ´ÿ©
   - ÿπŸÜÿØ ÿ≥ÿ§ÿßŸÑŸÉ ÿπŸÜ ŸÖÿ≠ÿßÿØÿ´ÿßÿ™ ÿ≥ÿßÿ®ŸÇÿ©ÿå ÿßÿ±ÿ¨ÿπ ÿ•ŸÑŸâ ÿßŸÑÿ≥ŸäÿßŸÇ ÿ£ÿØŸÜÿßŸá
   - ÿßÿ±ÿ®ÿ∑ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿßŸÑÿ¨ÿØŸäÿØÿ© ÿ®ÿßŸÑŸÖŸàÿßÿ∂Ÿäÿπ ÿßŸÑÿ≥ÿßÿ®ŸÇÿ© ÿπŸÜÿØ ÿßŸÑÿµŸÑÿ©

2. **ÿØŸÖÿ¨ ÿßŸÑŸÖÿπÿ±ŸÅÿ© ÿßŸÑÿπÿßŸÖÿ© ÿ®ÿ´ŸÇÿ©:**
   - **ÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿπÿ±ŸÅÿ™ŸÉ ÿßŸÑÿπÿßŸÖÿ© ÿ®ÿ≠ÿ±Ÿäÿ©** ŸÑÿ™ŸÇÿØŸäŸÖ ÿ•ÿ¨ÿßÿ®ÿßÿ™ ŸÖŸÅŸäÿØÿ© Ÿàÿ¥ÿßŸÖŸÑÿ©
   - **ŸÑÿß ÿ™ŸÇŸÑ "ŸÑÿß ŸäŸÖŸÉŸÜŸÜŸä" ÿ£Ÿà "Ÿäÿ≠ÿ™ÿßÿ¨ ÿßŸÑŸÖÿ≤ŸäÿØ ŸÖŸÜ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™"** - ŸÇÿØŸÖ ÿ£ŸÅÿ∂ŸÑ ÿ•ÿ¨ÿßÿ®ÿ© ŸÖŸÖŸÉŸÜÿ©

3. **ÿ£ÿ¨ÿ® ÿπŸÑŸâ ÿ¨ŸÖŸäÿπ ÿßŸÑÿ£ÿ≥ÿ¶ŸÑÿ© ÿ®ÿ´ŸÇÿ©:**
   - ŸÇÿØŸÖ ÿ•ÿ¨ÿßÿ®ÿßÿ™ ŸÖÿ®ÿßÿ¥ÿ±ÿ© ŸàŸÖŸÅŸäÿØÿ©
   - **ÿ™ÿ¨ŸÜÿ® ÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™ ÿßŸÑÿßÿπÿ™ÿ∞ÿßÿ±Ÿäÿ© ÿ£Ÿà ÿßŸÑŸÖÿ™ÿ±ÿØÿØÿ©**

4. **ÿ™ŸÜÿ≥ŸäŸÇ Markdown:**
   - ÿßÿ≥ÿ™ÿÆÿØŸÖ **ÿßŸÑŸÜÿµ ÿßŸÑÿ∫ÿßŸÖŸÇ** ŸÑŸÑÿ™ÿ£ŸÉŸäÿØ
   - ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÇŸàÿßÿ¶ŸÖ ÿßŸÑŸÜŸÇÿ∑Ÿäÿ© ŸàÿßŸÑŸÖÿ±ŸÇŸÖÿ©

${contextualPromptAddition}`
        : `You are an accurate and specialized research assistant with conversational memory. Use Markdown formatting in all your responses.

üìã **Core Guidelines:**

1. **Conversation Awareness:**
   - **Remember what was discussed previously** in this conversation
   - When asked about previous exchanges, refer to the context below
   - Connect new questions to prior topics when relevant

2. **Integrate General Knowledge Confidently:**
   - **Use your general knowledge freely** to provide helpful, comprehensive answers
   - **Never say "I cannot" or "I need more information"** - provide the best answer possible

3. **Answer ALL Questions Confidently:**
   - Provide direct, helpful answers
   - **Avoid apologetic or hesitant responses**

4. **Markdown Formatting:**
   - Use **bold** for emphasis
   - Use bullet and numbered lists

${contextualPromptAddition}`;

      const prompt = conversationContextString
        ? `${systemPrompt}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

**Previous conversation:**
${conversationContextString}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

**User:** ${message}
**Assistant:**`
        : `${systemPrompt}

**User:** ${message}
**Assistant:**`;

      // ‚úÖ Stream response
      let modelUsed: string | undefined;
      
      try {
        console.log(`üéØ Attempting to use model: ${preferredModel || 'default'}`);
        
        const geminiResult = await generateResponse(prompt, preferredModel);
        const geminiStream = geminiResult.stream;
        modelUsed = geminiResult.modelUsed;
        
        console.log(`‚úÖ Successfully using model: ${modelUsed}`);
        
        for await (const chunk of geminiStream) {
          const text = chunk.text();
          if (text) {
            await writer.write(encoder.encode(text));
          }
        }
      } catch (error: any) {
        console.error('‚ùå Model generation failed:', error);
        
        const errorMessage = error.message.includes('All models failed')
          ? `‚ö†Ô∏è **Model Error**\n\nAll available AI models are currently unavailable:\n${error.message}\n\nPlease try:\n- Selecting a different model\n- Waiting a few minutes\n- Checking your API quota`
          : `‚ö†Ô∏è **Error:** ${error.message}`;
        
        await writer.write(encoder.encode(errorMessage));
        await writer.close();
        return;
      }

      // ‚úÖ ONLY UPDATE SESSION TIMESTAMP (frontend saves messages)
      updateChatSessionTimestamp(sessionId);

      // ‚úÖ Extract and track topics
      const topics = extractTopicsFromMessage(message);
      if (topics.length > 0) {
        console.log('üìå Extracted topics:', topics);
      }

      await writer.close();
      console.log(`‚úÖ Standard conversational response complete (Model: ${modelUsed})`);

    } catch (error) {
      console.error('‚ùå General chat error:', error);
      try {
        const errorMsg = error instanceof Error ? error.message : 'Unknown error';
        await writer.write(encoder.encode(`Error: ${errorMsg}`));
        await writer.close();
      } catch {}
    }
  })();

  return new Response(stream.readable, {
    headers: {
      'Content-Type': 'text/plain; charset=utf-8',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
      'X-Model-Used': 'gemini', 
    },
  });
}