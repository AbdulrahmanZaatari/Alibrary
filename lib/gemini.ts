import { GoogleGenerativeAI } from '@google/generative-ai';

if (!process.env.GEMINI_API_KEY) {
  throw new Error('GEMINI_API_KEY not found');
}

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

// ‚úÖ Model hierarchy for fallback (best to worst)
const CHAT_MODELS = [
  'gemini-2.5-pro',
  'gemini-2.5-flash',
  'gemini-2.5-flash-lite',
  'gemini-2.0-flash'
];


const RERANK_MODELS = [
  'gemini-2.5-flash-lite',
  'gemini-2.0-flash',
  'gemini-2.0-flash-lite'
];

// Embed text using Gemini
export const embedText = async (text: string): Promise<number[]> => {
  const model = genAI.getGenerativeModel({ model: 'text-embedding-004' });
  const result = await model.embedContent(text);
  return result.embedding.values;
};

// ‚úÖ Generate response with streaming + model fallback
export const generateResponse = async (
  prompt: string,
  preferredModel?: string
): Promise<{ stream: AsyncIterable<any>; modelUsed: string }> => {
  const modelsToTry = preferredModel 
    ? [preferredModel, ...CHAT_MODELS.filter(m => m !== preferredModel)]
    : CHAT_MODELS;

  const errors: Array<{ model: string; error: string; reason: string }> = [];

  for (let i = 0; i < modelsToTry.length; i++) {
    const modelName = modelsToTry[i];
    try {
      console.log(`ü§ñ Trying model: ${modelName}`);
      
      const model = genAI.getGenerativeModel({ 
        model: modelName,
        generationConfig: {
          temperature: 0.7,
          maxOutputTokens: 8192,
        }
      });
      
      const result = await model.generateContentStream(prompt);
      console.log(`‚úÖ Success with ${modelName}`);
      
      return { 
        stream: result.stream, 
        modelUsed: modelName 
      };
      
    } catch (error: any) {
      const isQuotaError = error?.status === 429 || 
                          error?.message?.includes('quota') || 
                          error?.message?.includes('RESOURCE_EXHAUSTED');
      
      const isUnsupported = error?.status === 400 || 
                           error?.message?.includes('model not found') ||
                           error?.message?.includes('Invalid model');
      
      const isLastModel = i === modelsToTry.length - 1;
      
      let errorReason = 'Unknown error';
      if (isQuotaError) errorReason = 'Quota exceeded';
      else if (isUnsupported) errorReason = 'Model not available';
      else errorReason = error.message;

      errors.push({ model: modelName, error: error.message, reason: errorReason });
      
      if ((isQuotaError || isUnsupported) && !isLastModel) {
        console.warn(`‚ö†Ô∏è ${modelName} failed (${errorReason}), trying next model...`);
        continue;
      }
      
      console.error(`‚ùå ${modelName} failed:`, error.message);
      if (isLastModel) {
        const errorSummary = errors.map(e => `${e.model}: ${e.reason}`).join('\n');
        throw new Error(`All models failed:\n${errorSummary}`);
      }
    }
  }
  
  throw new Error('Failed to generate response with all available models');
};

// ‚úÖ PRODUCTION-GRADE CHUNKING FUNCTION
export const chunkText = (
  text: string, 
  chunkSize: number = 1200,
  overlap: number = 200
): string[] => {
  console.log('üì¶ Starting text chunking...');
  console.log(`   Original text length: ${text.length} characters`);

  // ‚úÖ Step 1: Clean and normalize text
  const cleanText = text
    .replace(/\s+/g, ' ')
    .replace(/[-_]+\s*\d+\s*[-_]+/g, '')
    .replace(/ÿµŸÅÿ≠ÿ©\s*\d+/g, '')
    .replace(/Page\s*\d+/gi, '')
    .replace(/[_-]{3,}/g, '')
    .replace(/[ÿ£ÿ•ÿ¢]/g, 'ÿß')
    .replace(/[Ÿâÿ¶]/g, 'Ÿä')
    .replace(/ÿ©/g, 'Ÿá')
    .replace(/\n{3,}/g, '\n\n')
    .trim();

  console.log(`   Cleaned text length: ${cleanText.length} characters`);

  const arabicSentenceEndings = /[.!?ÿü€î‡•§·Åã‡•§„ÄÇ]+/g;
  const sentences = cleanText.split(arabicSentenceEndings)
    .map(s => s.trim())
    .filter(s => s.length > 20);

  console.log(`   Found ${sentences.length} sentences`);

  const chunks: string[] = [];
  let currentChunk = '';

  for (let i = 0; i < sentences.length; i++) {
    const sentence = sentences[i];
    const testChunk = currentChunk + (currentChunk ? '. ' : '') + sentence;

    if (testChunk.length > chunkSize && currentChunk.length > 0) {
      if (isSubstantialContent(currentChunk)) {
        chunks.push(currentChunk.trim() + '.');
        console.log(`   ‚úì Created chunk ${chunks.length}: ${currentChunk.length} chars`);
      } else {
        console.log(`   ‚úó Skipped header-only chunk: "${currentChunk.substring(0, 50)}..."`);
      }

      const sentencesInChunk = currentChunk.split(/[.!?ÿü]+/).filter(s => s.length > 10);
      const overlapSentences = sentencesInChunk.slice(-3).join('. ');
      currentChunk = overlapSentences + (overlapSentences ? '. ' : '') + sentence;
    } else {
      currentChunk = testChunk;
    }
  }

  if (currentChunk.trim().length > 0 && isSubstantialContent(currentChunk)) {
    chunks.push(currentChunk.trim() + '.');
    console.log(`   ‚úì Created final chunk ${chunks.length}: ${currentChunk.length} chars`);
  }

  console.log(`‚úÖ Chunking complete: ${chunks.length} valid chunks created`);
  
  const avgLength = chunks.reduce((sum, c) => sum + c.length, 0) / chunks.length;
  const tooShort = chunks.filter(c => c.length < 200).length;
  
  console.log(`üìä Chunk Statistics:`);
  console.log(`   - Total chunks: ${chunks.length}`);
  console.log(`   - Average length: ${Math.round(avgLength)} chars`);
  console.log(`   - Chunks < 200 chars: ${tooShort}`);
  console.log(`   - Coverage: ${((chunks.join('').length / text.length) * 100).toFixed(1)}%`);

  return chunks;
};

export async function rerankChunks(
  originalQuery: string,
  chunks: any[],
  targetTopN: number = 10
): Promise<any[]> {
  if (chunks.length === 0) {
    return [];
  }

  console.log(`ü§ñ Starting Re-ranking: ${chunks.length} chunks for query: "${originalQuery}"`);

  const chunkList = chunks
    .map((chunk, index) => {
      const preview = chunk.chunk_text.substring(0, 400).replace(/\n/g, ' ');
      return `[${index}] (Page ${chunk.page_number}, Sim: ${((chunk.similarity || 0) * 100).toFixed(1)}%)\n${preview}...\n`;
    })
    .join('\n');

  // ‚úÖ Use fallback models for reranking
  for (const modelName of RERANK_MODELS) {
    try {
      console.log(`üîÑ Reranking with ${modelName}...`);
      
      const model = genAI.getGenerativeModel({ 
        model: modelName,
        generationConfig: {
          temperature: 0.1,
        }
      });

      const prompt = `You are a search relevance expert. Rank these text chunks by relevance to the query.

QUERY: "${originalQuery}"

CHUNKS (${chunks.length} total):
${chunkList}

Return ONLY a single flat JSON array of the top ${Math.min(targetTopN, chunks.length)} chunk indices (just numbers), ordered from most to least relevant.

CORRECT format: [5, 2, 10, 1, 8, 14, 3]
WRONG format: [[5], [2], [10]]

Your response (numbers only):`;

      const result = await model.generateContent(prompt);
      const text = result.response.text().trim();
      
      console.log(`üìÑ Reranker raw response: ${text.substring(0, 200)}`);

      let indices: number[] = [];
      
      try {
        const parsed = JSON.parse(text);
        if (Array.isArray(parsed)) {
          indices = parsed.flat().filter(i => typeof i === 'number');
        }
      } catch {
        const codeBlockMatch = text.match(/```(?:json)?\s*(\[[\s\S]*?\])\s*```/);
        if (codeBlockMatch) {
          const parsed = JSON.parse(codeBlockMatch[1]);
          indices = Array.isArray(parsed) ? parsed.flat().filter(i => typeof i === 'number') : [];
        } else {
          const arrayMatch = text.match(/\[[\d,\[\]\s]+\]/);
          if (arrayMatch) {
            const parsed = JSON.parse(arrayMatch[0]);
            indices = Array.isArray(parsed) ? parsed.flat().filter(i => typeof i === 'number') : [];
          }
        }
      }

      if (!Array.isArray(indices) || indices.length === 0) {
        console.warn(`‚ö†Ô∏è ${modelName} returned invalid indices, trying next model...`);
        continue;
      }

      const validIndices = indices.filter(i => i >= 0 && i < chunks.length);

      if (validIndices.length === 0) {
        console.warn(`‚ö†Ô∏è No valid indices from ${modelName}, trying next model...`);
        continue;
      }

      const uniqueIndices = [...new Set(validIndices)];
      const rerankedChunks = uniqueIndices
        .map(index => chunks[index])
        .filter(chunk => chunk);
        
      console.log(`‚úÖ Re-ranking complete with ${modelName}. Top ${rerankedChunks.length} chunks selected`);
      
      return rerankedChunks.slice(0, targetTopN);

    } catch (error: any) {
      const isQuotaError = error?.status === 429 || error?.message?.includes('quota');
      console.error(`‚ùå ${modelName} reranking failed: ${error.message}`);
      
      if (!isQuotaError) {
        break; // Non-quota error, don't try other models
      }
    }
  }

  console.warn('‚ö†Ô∏è All reranking models failed. Using original order.');
  return chunks.slice(0, targetTopN);
}

function isSubstantialContent(text: string): boolean {
  const trimmed = text.trim();
  
  if (trimmed.length < 150) return false;
  if (/^[-_\d\s.]+$/.test(trimmed)) return false;
  if (/^(ÿßŸÑÿ®ÿßÿ®|ÿßŸÑŸÅÿµŸÑ|Chapter|Section)\s+[\u0600-\u06FF\w\s]+$/i.test(trimmed)) return false;
  
  const sentenceCount = (trimmed.match(/[.!?ÿü]+/g) || []).length;
  if (sentenceCount < 3) return false;
  
  const wordCount = trimmed.split(/\s+/).length;
  if (wordCount < 30) return false;
  
  return true;
}

export default genAI;