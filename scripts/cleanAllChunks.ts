// ‚úÖ Load env FIRST before any imports
// eslint-disable-next-line @typescript-eslint/no-require-imports
require('dotenv').config({ 
  // eslint-disable-next-line @typescript-eslint/no-require-imports
  path: require('path').resolve(process.cwd(), '.env.local') 
});

import { createClient } from '@supabase/supabase-js';
import { GoogleGenerativeAI } from '@google/generative-ai';

// Verify env vars loaded
if (!process.env.GEMINI_API_KEY) {
  console.error('‚ùå GEMINI_API_KEY not found in environment');
  console.log('üìÇ Current directory:', process.cwd());
  console.log('üîç Looking for: .env.local');
  process.exit(1);
}

console.log('‚úÖ Environment variables loaded');

const supabaseAdmin = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_ROLE_KEY!
);

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);

const FALLBACK_MODELS = [
  'gemini-2.0-flash',
  'gemini-2.0-flash-lite', 
  'gemini-2.5-flash-lite',
  'gemini-2.5-flash',
];

/**
 * ‚úÖ Embed text using Gemini (inline version for script)
 */
async function embedText(text: string): Promise<number[]> {
  const model = genAI.getGenerativeModel({ model: 'text-embedding-004' });
  const result = await model.embedContent(text);
  return result.embedding.values;
}

/**
 * ‚úÖ COMPREHENSIVE Arabic text correction with OCR error fixing
 */
async function correctArabicChunk(text: string): Promise<string> {
  if (!text || text.length < 20) return text;

  const prompt = `ÿ£ŸÜÿ™ ÿÆÿ®Ÿäÿ± ŸÅŸä ÿ™ÿµÿ≠Ÿäÿ≠ ÿßŸÑŸÜÿµŸàÿµ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÖÿ≥ÿ™ÿÆÿ±ÿ¨ÿ© ŸÖŸÜ ŸÖŸÑŸÅÿßÿ™ PDF.

**ÿßŸÑŸÖŸáŸÖÿ©:** ÿµÿ≠Ÿëÿ≠ ÿ¨ŸÖŸäÿπ ÿßŸÑÿ£ÿÆÿ∑ÿßÿ° ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ™ÿßŸÑŸä:

1. **ÿ£ÿÆÿ∑ÿßÿ° OCR ÿßŸÑÿ¥ÿßÿ¶ÿπÿ©:**
   - "ŸÅŸÑŸÖÿ™" ‚Üí "ŸÅŸÑÿ´ŸÖÿ™" (ÿßŸÑÿ´ÿßÿ° ÿ™ÿµÿ®ÿ≠ ŸÑÿßŸÖ)
   - "ÿßÿ≥ŸÅŸãÿß" ‚Üí "ÿ¢ÿ≥ŸÅŸãÿß" (ÿßŸÑŸáŸÖÿ≤ÿ© ÿßŸÑŸÖŸÖÿØŸàÿØÿ©)
   - "ŸÅÿßŸäÿØŸá" ‚Üí "ŸÅÿßÿ¶ÿØÿ©" (ÿßŸÑŸáŸÖÿ≤ÿ© ÿπŸÑŸâ ÿßŸÑŸäÿßÿ°)
   - "ŸáÿßŸäÿ¨Ÿá" ‚Üí "Ÿáÿßÿ¶ÿ¨ÿ©" (ÿßŸÑŸáŸÖÿ≤ÿ© ÿπŸÑŸâ ÿßŸÑŸäÿßÿ°)
   - "ÿßŸÑŸÖŸÅÿßÿ¨ÿßŸá" ‚Üí "ÿßŸÑŸÖŸÅÿßÿ¨ÿ£ÿ©" (ÿßŸÑŸáŸÖÿ≤ÿ© ÿπŸÑŸâ ÿßŸÑÿ£ŸÑŸÅ)
   - "ÿ≥ŸÖÿßÿ≠Ÿá" ‚Üí "ÿ≥ŸÖÿßÿ≠ÿ©" (ÿßŸÑÿ™ÿßÿ° ÿßŸÑŸÖÿ±ÿ®Ÿàÿ∑ÿ©)
   - "ŸÇŸÅÿßŸá" ‚Üí "ŸÇŸÅÿßŸá" (ÿ≠ÿ≥ÿ® ÿßŸÑÿ≥ŸäÿßŸÇ)

2. **ÿßŸÑÿ£ÿÆÿ∑ÿßÿ° ÿßŸÑÿ•ŸÖŸÑÿßÿ¶Ÿäÿ©:**
   - ÿ•ÿµŸÑÿßÿ≠ ÿßŸÑŸáŸÖÿ≤ÿßÿ™ ÿßŸÑÿÆÿßÿ∑ÿ¶ÿ©
   - ÿ•ÿµŸÑÿßÿ≠ ÿßŸÑÿ™ÿßÿ° ÿßŸÑŸÖÿ±ÿ®Ÿàÿ∑ÿ© ŸàÿßŸÑŸáÿßÿ°
   - ÿ•ÿµŸÑÿßÿ≠ ÿßŸÑÿ£ŸÑŸÅ ÿßŸÑŸÖŸÇÿµŸàÿ±ÿ© ŸàÿßŸÑŸäÿßÿ°
   - ÿ•ÿµŸÑÿßÿ≠ ÿßŸÑÿ™ŸÜŸàŸäŸÜ ŸàÿßŸÑÿ™ÿ¥ŸÉŸäŸÑ

3. **ÿπŸÑÿßŸÖÿßÿ™ ÿßŸÑÿ™ÿ±ŸÇŸäŸÖ:**
   - ÿ•ÿµŸÑÿßÿ≠ ÿßŸÑŸÖÿ≥ÿßŸÅÿßÿ™ ŸÇÿ®ŸÑ Ÿàÿ®ÿπÿØ ÿπŸÑÿßŸÖÿßÿ™ ÿßŸÑÿ™ÿ±ŸÇŸäŸÖ
   - ÿ•ÿµŸÑÿßÿ≠ ÿßŸÑŸÜŸÇÿßÿ∑ ŸàÿßŸÑŸÅŸàÿßÿµŸÑ

**ŸÇŸàÿßÿπÿØ ŸÖŸáŸÖÿ©:**
- ÿßÿ≠ÿ™ŸÅÿ∏ ÿ®ÿßŸÑŸÖÿπŸÜŸâ ÿßŸÑÿ£ÿµŸÑŸä ÿ™ŸÖÿßŸÖŸãÿß
- ŸÑÿß ÿ™ÿ∫Ÿäÿ± ÿßŸÑÿ®ŸÜŸäÿ© ÿ£Ÿà ÿßŸÑÿ£ÿ≥ŸÑŸàÿ®
- ŸÑÿß ÿ™ÿ∂ŸÅ ŸÖÿ≠ÿ™ŸàŸâ ÿ¨ÿØŸäÿØ
- ÿ£ÿ±ÿ¨ÿπ ÿßŸÑŸÜÿµ ÿßŸÑŸÖÿµÿ≠ÿ≠ ŸÅŸÇÿ∑ ÿ®ÿØŸàŸÜ ÿ¥ÿ±ÿ≠

**ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä:**
${text}

**ÿßŸÑŸÜÿµ ÿßŸÑŸÖÿµÿ≠ÿ≠:**`;

  let lastError: Error | null = null;

  for (const modelName of FALLBACK_MODELS) {
    try {
      const model = genAI.getGenerativeModel({ 
        model: modelName,
        generationConfig: {
          temperature: 0.1,
          maxOutputTokens: 2000,
        }
      });

      const result = await model.generateContent(prompt);
      let corrected = result.response.text().trim();

      // Remove markdown formatting if AI adds it
      corrected = corrected.replace(/^```[\s\S]*?\n/, '').replace(/\n```$/, '');
      corrected = corrected.replace(/^\*\*ÿßŸÑŸÜÿµ ÿßŸÑŸÖÿµÿ≠ÿ≠:\*\*\s*/, '');

      // Validation: ensure similar length (¬±30%)
      const lengthDiff = Math.abs(corrected.length - text.length) / text.length;
      if (lengthDiff > 0.3) {
        console.warn(`   ‚ö†Ô∏è ${modelName} changed length too much (${(lengthDiff * 100).toFixed(1)}%), trying next model`);
        continue;
      }

      // Validation: ensure Arabic content preserved
      const originalArabic = (text.match(/[\u0600-\u06FF]/g) || []).length;
      const correctedArabic = (corrected.match(/[\u0600-\u06FF]/g) || []).length;
      const arabicDiff = Math.abs(correctedArabic - originalArabic) / originalArabic;

      if (arabicDiff > 0.2) {
        console.warn(`   ‚ö†Ô∏è ${modelName} removed too much Arabic (${(arabicDiff * 100).toFixed(1)}%), trying next model`);
        continue;
      }

      console.log(`   ‚úÖ Corrected with ${modelName}`);
      return corrected;

    } catch (error) {
      lastError = error as Error;
      console.warn(`   ‚ö†Ô∏è Correction failed with ${modelName}:`, error instanceof Error ? error.message.substring(0, 100) : 'Unknown');
      continue;
    }
  }

  console.error('   ‚ùå All correction models failed, returning original text');
  return text;
}

/**
 * ‚úÖ Detect if chunk needs correction
 */
function needsCorrection(text: string): boolean {
  const corruptionPatterns = [
    /ÿßÿ≥ŸÅŸãÿß/,          // Missing hamza madda
    /ŸÅÿßŸäÿØŸá/,          // Hamza on wrong letter
    /ŸáÿßŸäÿ¨Ÿá/,          // Hamza on wrong letter  
    /ÿßŸÑŸÖŸÅÿßÿ¨ÿßŸá/,       // Hamza on wrong letter
    /ÿ≥ŸÖÿßÿ≠Ÿá/,          // Ÿá instead of ÿ©
    /ŸÇŸÅÿßŸá/,           // Context-dependent
    /Ÿàÿ≤ÿ±Ÿë/,           // Spacing issues
    /ŸÖŸÜŸá ÿ®ÿØ\.\.\./,   // Spacing around punctuation
    /\s[ÿå.!ÿü]\s/,     // Spaces around Arabic punctuation
    /[ŸáŸâ]$/,          // Wrong ending letter (common OCR error)
    /\bŸÅÿßŸÑ([ÿß-Ÿä])/,   // "ŸÅÿßŸÑ" instead of "ŸÅŸÑÿß"
    /[ÿßÿ•ÿ¢]ŸÑ([ŸÄ-Ÿä])/,  // Hamza issues with "ÿßŸÑ"
  ];

  return corruptionPatterns.some(pattern => pattern.test(text));
}

/**
 * ‚úÖ Process chunks in batches with rate limiting
 */
async function cleanAllChunks(documentId?: string) {
  console.log('üßπ Starting comprehensive chunk cleaning...\n');

  // Get all chunks (or specific document)
  let query = supabaseAdmin
    .from('embeddings')
    .select('id, chunk_text, document_id, page_number, embedding');

  if (documentId) {
    query = query.eq('document_id', documentId);
    console.log(`üìÑ Cleaning document: ${documentId}`);
  } else {
    console.log('üìö Cleaning ALL documents');
  }

  const { data: chunks, error } = await query;

  if (error) {
    console.error('‚ùå Error fetching chunks:', error);
    return;
  }

  if (!chunks || chunks.length === 0) {
    console.log('‚ö†Ô∏è No chunks found');
    return;
  }

  console.log(`üìä Total chunks: ${chunks.length}\n`);

  // Filter chunks that need correction
  const corruptedChunks = chunks.filter(chunk => needsCorrection(chunk.chunk_text));
  
  console.log(`üîç Found ${corruptedChunks.length} potentially corrupted chunks\n`);

  if (corruptedChunks.length === 0) {
    console.log('‚úÖ No corrupted chunks found!');
    return;
  }

  let corrected = 0;
  let failed = 0;
  let skipped = 0;

  // Process in batches with rate limiting
  const batchSize = 5;
  const delayBetweenBatches = 3000; // 3 seconds

  for (let i = 0; i < corruptedChunks.length; i += batchSize) {
    const batch = corruptedChunks.slice(i, i + batchSize);
    const batchNum = Math.floor(i / batchSize) + 1;
    const totalBatches = Math.ceil(corruptedChunks.length / batchSize);

    console.log(`\nüì¶ Batch ${batchNum}/${totalBatches} (${batch.length} chunks)`);

    for (const chunk of batch) {
      try {
        console.log(`\n   Processing chunk ${chunk.id} (page ${chunk.page_number})...`);
        console.log(`   Original: ${chunk.chunk_text.substring(0, 80)}...`);

        // Correct the text
        const correctedText = await correctArabicChunk(chunk.chunk_text);

        // Check if text actually changed
        if (correctedText === chunk.chunk_text) {
          console.log('   ‚ö†Ô∏è No changes made, skipping');
          skipped++;
          continue;
        }

        console.log(`   Corrected: ${correctedText.substring(0, 80)}...`);

        // Re-generate embedding for corrected text
        console.log('   üîÑ Regenerating embedding...');
        const newEmbedding = await embedText(correctedText);

        // Update in database
        const { error: updateError } = await supabaseAdmin
          .from('embeddings')
          .update({
            chunk_text: correctedText,
            embedding: newEmbedding
          })
          .eq('id', chunk.id);

        if (updateError) {
          console.error(`   ‚ùå Failed to update:`, updateError);
          failed++;
        } else {
          console.log('   ‚úÖ Updated successfully');
          corrected++;
        }

        // Small delay between chunks
        await new Promise(resolve => setTimeout(resolve, 500));

      } catch (error) {
        console.error(`   ‚ùå Error processing chunk ${chunk.id}:`, error);
        failed++;
      }
    }

    // Delay between batches to respect rate limits
    if (i + batchSize < corruptedChunks.length) {
      console.log(`\n‚è≥ Waiting ${delayBetweenBatches / 1000}s before next batch...`);
      await new Promise(resolve => setTimeout(resolve, delayBetweenBatches));
    }
  }

  console.log(`\n\n‚úÖ Cleaning complete!`);
  console.log(`   Corrected: ${corrected}`);
  console.log(`   Skipped (no changes): ${skipped}`);
  console.log(`   Failed: ${failed}`);
}

// Run the script
const documentId = process.argv[2]; // Optional: pass document ID as argument
cleanAllChunks(documentId).catch(console.error);